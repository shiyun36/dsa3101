{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b38a7-2950-4936-9ffa-025243c15b21",
   "metadata": {},
   "source": [
    "# ESG financial Insights Generator \n",
    "\n",
    "The system will retrieve the latest ESG reports and regulatory documents to generate concicse, insightful summaries answering ESG-related queries for a specific ompany. \n",
    "\n",
    "Problem statement: How can a tool summarise ESG related news of each company? \n",
    "\n",
    "Background: \n",
    "This is a continuation from assignment1.Based on these newly sourced company ESG reports, and the the previous classification method from assignment1, I can classify these chunks of text into the E,S and G categories, so that model is able to better generate a response related to one of the particular categories. For example, the query could be \"What are the recent carbon emission goals of Pfizer?\" Since in the data loading portion of this assignmnet I have added Pfizer's ESG report that details ESG strategies, and \"carbon emission\" is under the \"environmental\" category, based on these contexts, the model is able to get the specific document chunk context and metadata (the categories) to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd5e659-63fd-4355-a5ad-b5c38160fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "### if u got strong gpu w cuda i recommend changing to gpu, average laptop cpu takes too long *cough* mac book users\n",
    "torch.set_default_device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"running on cuda\")\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "google_api_key = \"AIzaSyCutzQsZEOJUQgHwcvjtPNiLFbgyxOfmko\"\n",
    "from openai import OpenAI\n",
    "API_KEY = \"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\" \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bee610-2d14-4f3e-9bc3-4f163c62175e",
   "metadata": {},
   "source": [
    "# Data Sourcing & Preprocessing\n",
    "- ESG news articles (This was pulled from the previous project, using APIs and webscraping, so I transferred the downloaded files over.)\n",
    "- Company ESG pdf reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c97ce-fc30-4bce-8af1-21396f3b56db",
   "metadata": {},
   "source": [
    "## Automate pulling pdfs online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3619c33-dde8-4cda-b3ec-074c8cf2926f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a randomly generated list of company names with varying sizes, industries, and countries:\n",
      "\n",
      "1.  **Samsung** (South Korea, Technology, Large)\n",
      "2.  **Nestlé** (Switzerland, Food & Beverage, Large)\n",
      "3.  **Tata Consultancy Services (TCS)** (India, IT Services, Large)\n",
      "4.  **Adidas** (Germany, Apparel & Sportswear, Large)\n",
      "5.  **Toyota** (Japan, Automotive, Large)\n",
      "6.  **Shopify** (Canada, E-commerce, Large)\n",
      "7.  **Eni** (Italy, Energy, Large)\n",
      "8.  **Banco Santander** (Spain, Banking, Large)\n",
      "9.  **Woolworths Group** (Australia, Retail, Large)\n",
      "10. **Unilever** (United Kingdom/Netherlands, Consumer Goods, Large)\n",
      "11. **Novo Nordisk** (Denmark, Pharmaceutical, Large)\n",
      "12. **Siemens** (Germany, Manufacturing, Large)\n",
      "13. **BP** (United Kingdom, Energy, Large)\n",
      "14. **TikTok (ByteDance)** (China, Social Media, Large)\n",
      "15. **Hermès** (France, Luxury Goods, Large)\n",
      "\n",
      "['Samsung', 'Nestlé', 'Tata Consultancy Services (TCS)']\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=google_api_key)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "gemini_eval = model.generate_content(\n",
    "    contents= \"Can you randomly generate a list of company names that come from different sizes, based in different countries, and of various industries. For example, companies that could be generated are [Pfizer, Apple, Uber, LVMH], etc where they are in the healthcare, technology, transport, FMCG and based in different countries. In your response, number the list like so 1.Pfizer, 2.Apple \"\n",
    ")\n",
    "print(gemini_eval.text)\n",
    "company_names = [name for name in re.findall(r\"\\*\\*(.+?)\\*\\*\", gemini_eval.text)  if \"Disclaimer\" not in name]\n",
    "company_list = company_names[:3]\n",
    "years=[2024, 2023]\n",
    "print(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e2a158-070f-4add-a20b-42128b8dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.samsung.com/global/sustainability/media/pdf/Samsung_Electronics_Sustainability_Report_2024_ENG.pdf', 'https://www.samsung.com/global/sustainability/media/pdf/Samsung_Electronics_Sustainability_Report_2023_ENG.pdf', 'https://www.nestle.com/sites/default/files/2025-02/non-financial-statement-2024.pdf', 'https://www.nestle.com/sites/default/files/2024-02/creating-shared-value-sustainability-report-2023-en.pdf', 'https://www.tcs.com/content/dam/tcs/pdf/discover-tcs/corporate-sustainability/tcs-carbon-reduction-plan-aug-2024.pdf', 'https://www.tcs.com/content/dam/global-tcs/en/pdfs/investors/esg/2023-24/fy24-qes-mea.pdf']\n"
     ]
    }
   ],
   "source": [
    "pdf_links = []\n",
    "for company in company_list:\n",
    "    for year in years:\n",
    "        query = f\"{company} {year} ESG report filetype:pdf\"\n",
    "        \n",
    "        for url in search(query, num_results=1):\n",
    "            if url.endswith(\".pdf\"):  \n",
    "                pdf_links.append(url)\n",
    "print(pdf_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f601f57-c062-43e9-b304-205d55fe2dfc",
   "metadata": {},
   "source": [
    "### pdf image to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ae95a9-7874-456e-a0e1-f89fab8aa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_and_year(pdf_url):\n",
    "    \"\"\"\n",
    "    Extract the company name and year from the URL or filename.\n",
    "    Assuming the URL contains the company name and year in a predictable format like: \n",
    "    Assumes company name is right after 'https://' and the year is just before '.pdf'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'https://(?:www\\.)?([a-zA-Z0-9-]+).*?(\\d{4}(?:-\\d{4})?)\\.pdf', pdf_url)\n",
    "    if match:\n",
    "        company_name = match.group(1)  # The company name\n",
    "        year = match.group(2)  # The year\n",
    "        return company_name, year\n",
    "    else:\n",
    "        print(f\"Failed to name and year extract from URL: {pdf_url}\")\n",
    "        return None, None\n",
    "\n",
    "def process_pdf_from_url(pdf_url, output_pdf_path):\n",
    "    \"\"\"\n",
    "    Processes a PDF from a URL, applies OCR, and saves the OCR version.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_pdf_path):\n",
    "        print(f\"Skipping '{pdf_url}': already processed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing '{pdf_url}'...\")\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        pdf_data = BytesIO(response.content)  # Convert the PDF to BytesIO stream\n",
    "\n",
    "        # Apply OCR to the PDF data\n",
    "        ocrmypdf.ocr(pdf_data, output_pdf_path, force_ocr=True)\n",
    "        print(f\"Saved OCR version to '{output_pdf_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{pdf_url}': {e}\")\n",
    "\n",
    "def process_pdfs_from_urls(pdf_urls, output_root):\n",
    "    for pdf_url in pdf_urls:\n",
    "        company_name, year = extract_company_and_year(pdf_url)\n",
    "        print(f\"Processing PDF from compay: {company_name}, with URL: {pdf_url}\")\n",
    "        output_pdf_path = os.path.join(output_root, f\"{pdf_url.split('/')[-1].split('.')[0]}_ocr.pdf\")\n",
    "        process_pdf_from_url(pdf_url, output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26082c96-a5ce-4901-97f3-de4e171b2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to name and year extract from URL: https://www.samsung.com/global/sustainability/media/pdf/Samsung_Electronics_Sustainability_Report_2024_ENG.pdf\n",
      "Processing PDF from compay: None, with URL: https://www.samsung.com/global/sustainability/media/pdf/Samsung_Electronics_Sustainability_Report_2024_ENG.pdf\n",
      "Processing 'https://www.samsung.com/global/sustainability/media/pdf/Samsung_Electronics_Sustainability_Report_2024_ENG.pdf'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b55bb3803124bab9b12d6551219b164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This PDF has a fillable form. Chances are it is a pure digital document that does not need OCR.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9259361043447da186d025a7e8e72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[tesseract] lots of diacritics - possibly poor OCR\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[tesseract] lots of diacritics - possibly poor OCR\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[tesseract] lots of diacritics - possibly poor OCR\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[tesseract] lots of diacritics - possibly poor OCR\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">page has no images - all vector content will be rasterized at 400 DPI, losing some resolution and likely increasing\n",
       "file size. Use --oversample to adjust the DPI.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "page has no images - all vector content will be rasterized at 400 DPI, losing some resolution and likely increasing\n",
       "file size. Use --oversample to adjust the DPI.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some input metadata could not be copied because it is not permitted in PDF/A. You may wish to examine the output PDF's XMP metadata.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\PIL\\ImageFile.py:547\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    548\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_root):\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_root)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mprocess_pdfs_from_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m, in \u001b[0;36mprocess_pdfs_from_urls\u001b[1;34m(pdf_urls, output_root)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing PDF from compay: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m output_pdf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_root, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ocr.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mprocess_pdf_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m, in \u001b[0;36mprocess_pdf_from_url\u001b[1;34m(pdf_url, output_pdf_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     pdf_data \u001b[38;5;241m=\u001b[39m BytesIO(response\u001b[38;5;241m.\u001b[39mcontent)  \u001b[38;5;66;03m# Convert the PDF to BytesIO stream\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply OCR to the PDF data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mocrmypdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_ocr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved OCR version to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_pdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\api.py:375\u001b[0m, in \u001b[0;36mocr\u001b[1;34m(input_file, output_file, language, image_dpi, output_type, sidecar, jobs, use_threads, title, author, subject, keywords, rotate_pages, remove_background, deskew, clean, clean_final, unpaper_args, oversample, remove_vectors, force_ocr, skip_text, redo_ocr, skip_big, optimize, jpg_quality, png_quality, jbig2_lossy, jbig2_page_group_size, jbig2_threshold, pages, max_image_mpixels, tesseract_config, tesseract_pagesegmode, tesseract_oem, tesseract_thresholding, pdf_renderer, tesseract_timeout, tesseract_non_ocr_timeout, tesseract_downsample_above, tesseract_downsample_large_images, rotate_pages_threshold, pdfa_image_compression, color_conversion_strategy, user_words, user_patterns, fast_web_view, continue_on_soft_render_error, invalidate_digital_signatures, plugins, plugin_manager, keep_temporary_files, progress_bar, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m options \u001b[38;5;241m=\u001b[39m create_options(\n\u001b[0;32m    369\u001b[0m     input_file\u001b[38;5;241m=\u001b[39minput_file,\n\u001b[0;32m    370\u001b[0m     output_file\u001b[38;5;241m=\u001b[39moutput_file,\n\u001b[0;32m    371\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcreate_options_kwargs,\n\u001b[0;32m    373\u001b[0m )\n\u001b[0;32m    374\u001b[0m check_options(options, plugin_manager)\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\_pipelines\\ocr.py:225\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(options, plugin_manager)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m(\n\u001b[0;32m    214\u001b[0m     options: argparse\u001b[38;5;241m.\u001b[39mNamespace,\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    216\u001b[0m     plugin_manager: OcrmypdfPluginManager,\n\u001b[0;32m    217\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExitCode:\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the OCR pipeline without command line exception handling.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m            created.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\_pipelines\\ocr.py:192\u001b[0m, in \u001b[0;36m_run_pipeline\u001b[1;34m(options, plugin_manager)\u001b[0m\n\u001b[0;32m    189\u001b[0m validate_pdfinfo_options(context)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m optimize_messages \u001b[38;5;241m=\u001b[39m \u001b[43mexec_concurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m exitcode \u001b[38;5;241m=\u001b[39m report_output_pdf(options, start_input_file, optimize_messages)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exitcode\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\_pipelines\\ocr.py:148\u001b[0m, in \u001b[0;36mexec_concurrent\u001b[1;34m(context, executor)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39moutput_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# PDF/A and metadata\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostprocessing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m     pdf, messages \u001b[38;5;241m=\u001b[39m \u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Copy PDF file to destination\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     copy_final(pdf, options\u001b[38;5;241m.\u001b[39moutput_file, options\u001b[38;5;241m.\u001b[39minput_file)\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\_pipelines\\_common.py:427\u001b[0m, in \u001b[0;36mpostprocess\u001b[1;34m(pdf_file, context, executor)\u001b[0m\n\u001b[0;32m    424\u001b[0m save_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinearize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m optimizing \u001b[38;5;129;01mand\u001b[39;00m should_linearize(pdf_out, context)\n\u001b[0;32m    426\u001b[0m pdf_out \u001b[38;5;241m=\u001b[39m metadata_fixup(pdf_out, context, pdf_save_settings\u001b[38;5;241m=\u001b[39msave_settings)\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\_pipeline.py:894\u001b[0m, in \u001b[0;36moptimize_pdf\u001b[1;34m(input_file, context, executor)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optimize the given PDF file.\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m output_file \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimize.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 894\u001b[0m output_pdf, messages \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplugin_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinearize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_linearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m ratio, savings \u001b[38;5;241m=\u001b[39m _file_size_ratio(input_file, output_file)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ratio:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pluggy\\_hooks.py:513\u001b[0m, in \u001b[0;36mHookCaller.__call__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m firstresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# Copy because plugins may register other plugins during iteration (#438).\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hookexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hookimpls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirstresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pluggy\\_manager.py:120\u001b[0m, in \u001b[0;36mPluginManager._hookexec\u001b[1;34m(self, hook_name, methods, kwargs, firstresult)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hookexec\u001b[39m(\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    113\u001b[0m     hook_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# called from all hookcaller instances.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# enable_tracing will set its own wrapping function at self._inner_hookexec\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_hookexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirstresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\builtin_plugins\\optimize.py:145\u001b[0m, in \u001b[0;36moptimize_pdf\u001b[1;34m(input_pdf, output_pdf, context, executor, linearize)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;129m@hookimpl\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_pdf\u001b[39m(\n\u001b[0;32m    135\u001b[0m     input_pdf: Path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     linearize: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Path, Sequence[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    141\u001b[0m     save_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    142\u001b[0m         linearize\u001b[38;5;241m=\u001b[39mlinearize,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_pdf_save_settings(context\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39moutput_type),\n\u001b[0;32m    144\u001b[0m     )\n\u001b[1;32m--> 145\u001b[0m     result_path \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     messages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39moptimize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\optimize.py:692\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(input_file, output_file, context, save_settings, executor)\u001b[0m\n\u001b[0;32m    689\u001b[0m root \u001b[38;5;241m=\u001b[39m output_file\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    690\u001b[0m root\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 692\u001b[0m jpegs, pngs \u001b[38;5;241m=\u001b[39m \u001b[43mextract_images_generic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m transcode_jpegs(pdf, jpegs, root, options, executor)\n\u001b[0;32m    694\u001b[0m deflate_jpegs(pdf, root, options, executor)\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\optimize.py:343\u001b[0m, in \u001b[0;36mextract_images_generic\u001b[1;34m(pdf, root, options)\u001b[0m\n\u001b[0;32m    341\u001b[0m jpegs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    342\u001b[0m pngs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, xref_ext \u001b[38;5;129;01min\u001b[39;00m extract_images(pdf, root, options, extract_image_generic):\n\u001b[0;32m    344\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, xref_ext)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xref_ext\u001b[38;5;241m.\u001b[39mext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\optimize.py:323\u001b[0m, in \u001b[0;36mextract_images\u001b[1;34m(pdf, root, options, extract_fn)\u001b[0m\n\u001b[0;32m    321\u001b[0m image \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mget_object((xref, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxref \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: While extracting this image, an error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    329\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\ocrmypdf\\optimize.py:219\u001b[0m, in \u001b[0;36mextract_image_generic\u001b[1;34m(pdf, root, image, xref, options)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pim\u001b[38;5;241m.\u001b[39mindexed \u001b[38;5;129;01mand\u001b[39;00m pim\u001b[38;5;241m.\u001b[39mcolorspace \u001b[38;5;129;01min\u001b[39;00m pim\u001b[38;5;241m.\u001b[39mSIMPLE_COLORSPACES:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# An optimization opportunity here, not currently taken, is directly\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# generating a PNG from compressed data\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[43mpim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpng_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxref\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDF contains an atypical image that cannot be optimized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\PIL\\Image.py:2568\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[0;32m   2567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2568\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\PIL\\PngImagePlugin.py:1431\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1428\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[0;32m   1429\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\PIL\\ImageFile.py:551\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    549\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 551\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    553\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\PIL\\ImageFile.py:570\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 570\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    571\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_root = r\"datasets\"\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)\n",
    "process_pdfs_from_urls(pdf_links, output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132b2d-6218-424d-96ef-e5d5b23eb5c8",
   "metadata": {},
   "source": [
    "### pdf to text reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0196a771-4f20-4a7d-958d-3b55eca594a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ocr_pdf_to_text(input_pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from an OCR-based PDF, clean it, and save as a JSON list of sentences.\n",
    "    \"\"\"\n",
    "    def clean_page_text(text):\n",
    "        \"\"\"\n",
    "        Remove headers, footers, and page numbers from extracted text.\n",
    "        \"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        if len(lines) > 2:\n",
    "            lines = lines[2:]\n",
    "        lines = [line for line in lines if not re.match(r'^(Page\\s*\\d+|\\d+|P\\.\\s*\\d+)$', line.strip(), re.IGNORECASE)]\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    output_file = os.path.splitext(input_pdf_path)[0] + \".json\"\n",
    "    cleaned_text = ''\n",
    "    pdfReader = PdfReader(input_pdf_path)\n",
    "\n",
    "    for i in range(len(pdfReader.pages)):\n",
    "        pageObj = pdfReader.pages[i]\n",
    "        raw_text = pageObj.extract_text()\n",
    "        \n",
    "        if raw_text:\n",
    "            processed_text = clean_page_text(raw_text)\n",
    "            cleaned_text += processed_text + ' '  # separate pages with newlines\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', cleaned_text.strip())\n",
    "    final_sentences = [re.sub(r'\\n', ' ', sentence) for sentence in sentences] # places one sentence in a line\n",
    "    final_sentences = [re.sub(r'\\s+', ' ', sentence).strip() for sentence in sentences] # remove extra unnecessary white spaces\n",
    "\n",
    "    with open(output_file, \"w\", encoding = \"utf-8\") as f:\n",
    "        json.dump(final_sentences, f, indent = 4, ensure_ascii = False)\n",
    "    \n",
    "    print(f\"Sentences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e9fae0-d85a-4c2c-93bb-240abbd2275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"datasets/\"\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        input_pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        convert_ocr_pdf_to_text(input_pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c629fc8-9567-4747-9fb0-09cf5dd8cb50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Retrieve raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bde8414-62eb-486b-b811-cf065a65016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "pdf_folder_path = \"datasets/\"\n",
    "\n",
    "for folder_name in os.listdir(pdf_folder_path):\n",
    "    folder_path = os.path.join(pdf_folder_path, folder_name)  \n",
    "    if os.path.isdir(folder_path):  \n",
    "        json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "        for file in json_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)  # Load JSON file\n",
    "            if isinstance(data, list):  \n",
    "                df = pd.DataFrame(data, columns=[\"esg_text\"])\n",
    "            else:\n",
    "                print(f\"Skipping {file}: Unsupported format\")\n",
    "                continue\n",
    "            \n",
    "            dfs[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6d1ad6e-4baa-40d9-959c-f000162bc7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ea3284d-8f70-491b-a7d2-35451624f1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m         df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(year)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames into df_combined\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m missing_rows \u001b[38;5;241m=\u001b[39m df_combined[df_combined\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(missing_rows))\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "for file, df in dfs.items():\n",
    "    match = re.match(r\"(.+?)_(\\d{4})_ocr.json\", file)  \n",
    "    if match:\n",
    "        company_name, year = match.groups()\n",
    "        df[\"Company\"] = company_name\n",
    "        df[\"Year\"] = int(year)\n",
    "\n",
    "# Concatenate all DataFrames into df_combined\n",
    "df_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "missing_rows = df_combined[df_combined.isnull().any(axis=1)]\n",
    "print(len(missing_rows))\n",
    "df_combined.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94867c09-512d-43ca-a98e-6bbf61b55b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esg_text\n",
       "<class 'str'>    1193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"esg_text\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6022ba-6ae2-4953-a77c-2671fca48332",
   "metadata": {},
   "source": [
    "# RAG Pipeline Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229545c5-dbb3-487d-9322-5c38441a2d3a",
   "metadata": {},
   "source": [
    "## Data Loading \n",
    "Data loading methodology: Sentence splitting, saved into each row of the csv \n",
    "Use ChromaDB for vector database: Generate embeddings for each sentence, store embeddings in vector DB, and generate embedding for the query to retrieve top-k similar chunks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c99e0-fb4a-4186-b21c-f152da2d2c11",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "When you store documents in ChromaDB using collection.add(), it:\n",
    "\n",
    "1. Generates vector embeddings for your text (if you haven't provided them).\n",
    "2. Stores the document along with its embedding in the vector database.\n",
    "3. Matches queries based on similarity search (cosine similarity by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73636d00-c3d8-4f2b-8a48-5b53c736ab73",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '..\\x0ciles\\\\labeled_pdfs_1003'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;130;43;01m\\f\u001b[39;49;00m\u001b[38;5;124;43miles\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlabeled_pdfs_1003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '..\\x0ciles\\\\labeled_pdfs_1003'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_pdfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8abda4-601d-40e9-8e35-4e119b5b8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa4265_ass2\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]  \n",
    "    doc_company = row[\"Company\"]  \n",
    "    doc_year = row[\"Year\"]  \n",
    "    doc_id = f\"doc_{index}\"  \n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id], \n",
    "        documents=[doc_text],  \n",
    "        metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dadc0f-2077-4cf3-81ed-e63b76e2b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection.count())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b911e81-79af-4529-a8f3-ecfbc0e1ec24",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "Method: Semantic similarity to compare embeddings of the query to the sentences, and retrieve the top sentences with the highest similarity scores.\n",
    "Limitations: Different words may have different meanings under different contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c60d83-b651-4376-8f65-98f0f14c1c91",
   "metadata": {},
   "source": [
    "### Retrieval & Query Handling\n",
    "\n",
    "Hybrid Search pipeline\n",
    "\n",
    "Step 1: Vector Search in ChromaDB → Retrieve top-k relevant documents based on semantic similarity.\n",
    "\n",
    "Step 2: BGE Reranker Search -> Evaluates how relevant the retrieved documents are by comparing each document with the query, scoring them, and reordering them according to relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41199cad-f303-40cf-bbdf-8a90e8ddd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"dsa4265_ass2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8642cd89-d3ef-4160-981d-9ca5a5c7a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the carbon emission goals of pfizer\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    where={\"company\": \"Pfizer\"},\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e68400-2765-4e09-85d5-3eeaef3fe868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_10665', 'doc_10666', 'doc_9646', 'doc_9655', 'doc_9648']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.',\n",
       "   'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.',\n",
       "   'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.',\n",
       "   'Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040’.',\n",
       "   'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022}]],\n",
       " 'distances': [[0.5328716039657593,\n",
       "   0.5343808531761169,\n",
       "   0.5423866510391235,\n",
       "   0.5645685195922852,\n",
       "   0.6181094646453857]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788fa6f-c0bf-4c0d-bc1c-429fc4ff4659",
   "metadata": {},
   "source": [
    "### Reranking\n",
    "Rerank documents using bge-reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "799274ff-1f69-44de-a767-9445d6fb91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Docs: ['Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040’.', 'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.', 'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.', 'As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.', 'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "model_name = \"BAAI/bge-reranker-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [query] + retrieved_docs,  \n",
    "    padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Compute relevance scores\n",
    "with torch.no_grad():\n",
    "    scores = model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "# Sort docs by relevance score\n",
    "reranked_docs = [doc for _, doc in sorted(zip(scores[1:], retrieved_docs), reverse=True)]\n",
    "print(\"Reranked Docs:\", reranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d927e8-1983-44ab-89d2-b039e98ac88f",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Use DeepSeek API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abb83d1f-32ca-484d-a33e-693229822f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "643b776a-11c2-4abc-9259-5e6403ece55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, reranked_docs):\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1-zero:free\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in ESG analysis. Answer factually and ensure consistency with the provided context, especially focusing on environemntal, sustainability and governance principles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if completion and completion.choices and completion.choices[0].message:\n",
    "                return completion.choices[0].message.content  # Return model's response\n",
    "\n",
    "            print(\"Warning: Empty response. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"Error: Unable to generate a response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25449c11-be13-40b3-ad53-82a46f9e4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Generated Answer:\n",
      " \\boxed{Pfizer aims to reduce its company (Scope 1 & 2) GHG emissions by 46% by 2030 from a 2019 baseline. By 2040, Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) GHG emissions and a 90% reduction in value chain (Scope 3) emissions from 2019 levels.}\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(query, reranked_docs)\n",
    "print(\"\\n🔹 Generated Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da8a7d-aaf9-47bc-aac1-b76dda656a22",
   "metadata": {},
   "source": [
    "### Generating a response from Deepseek without the retrieval step's context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79869882-f27a-47b4-afd9-a9424231d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_without_context(query):\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a factually accurate response. \n",
    "    \"\"\"\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1-zero:free\", #deepseek-reasoner #deepseek/deepseek-r1:free\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in ESG analysis. Answer factually and ensure consistency with the provided context, especially focusing on environemntal, sustainability and governance principles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if completion and completion.choices and completion.choices[0].message:\n",
    "                return completion.choices[0].message.content  \n",
    "\n",
    "            print(\"Warning: Empty response. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"Error: Unable to generate a response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c928cc91-7c8b-4c69-a20d-daee1d1e95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\boxed{\\n\"Pfizer aims to reduce Scope 1 GHG emissions by 46% by 2030 from a 2019 baseline and Scope 2 GHG emissions by 20% by the same year from the same baseline. Additionally, the company aims for a 90% absolute reduction in operational Scope 1 and Scope 2 GHG emissions combined by 2040 from a 2019 baseline.\"\\n}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_without_context(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d54a7-83f2-4638-aa9f-2fb069a0f738",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "Checking for hallucination, irrelevance, bias \n",
    "In this assignment, I felt that biasness wasn't really a metric required, I think it would be good to add biasness if i extracted data from third party sources grading the company esg scores. I can then compare the third-party metrics and scoring to each company's esg reports, and check if there is biasness in terms of their ratings, towards a particular, company or industry, etc. Therefore, I just added the metric for future reference, but it is not required in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047809e-b41f-4a4f-a583-94f731705305",
   "metadata": {},
   "source": [
    "### Hallucination detection (Faithfullness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7e5914-e633-44c0-996d-1a3471006ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def fuzzy_match(sentence, doc, threshold=80):\n",
    "    \"\"\"Check if sentence has a fuzzy match in the document.\"\"\"\n",
    "    return fuzz.partial_ratio(normalize_text(sentence), normalize_text(doc)) >= threshold\n",
    "\n",
    "def verify_facts(response, reranked_docs, fuzzy_threshold=80):\n",
    "    \"\"\"Detect hallucinations by checking if sentences exist in retrieved docs using fuzzy matching.\"\"\"\n",
    "    missing_facts = []\n",
    "    \n",
    "    # Split response into sentences and check if they appear in any of the documents\n",
    "    for sent in response.split(\". \"):\n",
    "        found = any(fuzzy_match(sent, doc, fuzzy_threshold) for doc in reranked_docs)\n",
    "        if not found:\n",
    "            missing_facts.append(sent)\n",
    "\n",
    "    if missing_facts:\n",
    "        print(\"Warning: Some statements are not found in the retrieved context:\")\n",
    "        for fact in missing_facts:\n",
    "            print(f\"- {fact}\")\n",
    "    \n",
    "    return 1 - len(missing_facts) / len(response.split(\". \"))  # Faithfulness Score\n",
    "\n",
    "faithfulness_score = verify_facts(response, reranked_documents)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e383-f6c2-4755-86c6-db9fcfc00bc5",
   "metadata": {},
   "source": [
    "## Irrelevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda28345-5526-41e5-8d04-1e3cb97f9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def check_relevance(query, response, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Check the relevance of the response to the query using semantic similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    response_embedding = model.encode([response])\n",
    "\n",
    "    similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        return True, similarity  \n",
    "    else:\n",
    "        return False, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0730c8dd-2b62-4e30-bbe9-51973dacffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the response relevant? (True, 0.6938479)\n"
     ]
    }
   ],
   "source": [
    "is_relevant = check_relevance(query, response)\n",
    "print(f\"Is the response relevant? {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c597bf1-8a94-4e40-99da-74907eaa51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the response biased? False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-reranker-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def check_bias(text):\n",
    "    \"\"\"\n",
    "    Check for potential bias in the text using a pretrained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Assuming binary classification (0 = no bias, 1 = biased)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class == 1  # 1 indicates bias (this depends on the model's labeling)\n",
    "\n",
    "# Example Usage\n",
    "response = \"Pfizer has been focusing on improving diversity in their clinical trials and sharing their insights with others as part of their diversity and inclusion initiatives in 2022.\"\n",
    "\n",
    "is_biased = check_bias(response)\n",
    "print(f\"Is the response biased? {is_biased}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50fbe5b2-3250-48ec-a934-b75f3e54c158",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa267f2-b321-4758-afac-a20d44832149",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "Typical metrics: RecalL@k, Precision @k, Mean Reciprocal Rank, Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab63011-7877-436b-a499-c268b97fbeb4",
   "metadata": {},
   "source": [
    "### Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370e094-958d-46e4-9ef1-441dd6c21acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(reranked_docs, ground_truth_docs, k=5):\n",
    "    \"\"\"Compute Recall@K: how many relevant docs were retrieved.\"\"\"\n",
    "    relevant_retrieved = sum(1 for doc in reranked_docs[:k] if doc in ground_truth_docs)\n",
    "    return relevant_retrieved / len(ground_truth_docs) if ground_truth_docs else 0\n",
    "\n",
    "recall_at_k(reranked_docs, ground_truth_docs, k=5) ## Need to manually label the ground truth docs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5740f5c-7620-48e9-820d-1ebf2e5f7026",
   "metadata": {},
   "source": [
    "### MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5cada-0fca-4381-a5f4-2eb8e75bda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(reranked_docs, ground_truth_docs):\n",
    "    \"\"\"Compute MRR: rewards ranking of first relevant document.\"\"\"\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        if doc in ground_truth_docs:\n",
    "            return 1 / (i + 1)  # Rank starts from 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc47e47-9939-43d9-8c8d-e0130e7be26f",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5510ac0-4b18-4606-a1b4-b155d3634525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(query, reranked_docs):\n",
    "    \"\"\"Compute semantic similarity between query and retrieved docs.\"\"\"\n",
    "    corpus = [query] + reranked_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    return similarity_scores.mean()  # Average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea71078-d266-46bf-9106-943f2dd30a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09264488847701834"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_retrieval_relevance(query, reranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f70ad-88b8-4692-a72c-775ba45db831",
   "metadata": {},
   "source": [
    "## Generator Evaluation \n",
    "Typical metrics: ROUGE, BLEU, BERTScore, domain-specific or task-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410181f7-bc69-415c-991c-5461bdb5b27b",
   "metadata": {},
   "source": [
    "### BLEU Score (Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5f9b46e-b114-427a-a366-d5c22324a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(reference, generated_response):\n",
    "    \"\"\"Compare generated response against reference text using BLEU score.\"\"\"\n",
    "    reference_tokens = reference.lower().split()\n",
    "    generated_tokens = generated_response.lower().split()\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3c5aae-8086-4306-b318-d697e93cccfa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.50440384721771e-232"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu_score(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb8f94-3737-4fcf-b003-74a17bfa839f",
   "metadata": {},
   "source": [
    "### Retrieval score (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6208a-09cd-4eb8-9523-b78e3152a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(reranked_docs, response):\n",
    "    \"\"\"Calculate how relevant the response is to the retrieved documents.\"\"\"\n",
    "    corpus = reranked_docs + [response]  # Combine all docs and response\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    return similarity_matrix.mean()  # Average similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c430da-47b0-464a-a510-d5a3ac0c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(reranked_docs, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4a61-3080-4ed3-929f-6560fe92648a",
   "metadata": {},
   "source": [
    "### Judge LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aab3e25-334e-4c48-82f4-d9a27b677207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response accurately and completely answers the query about Pfizer's carbon emission goals. It provides both the 2030 and 2040 targets for Scope 1 & 2 emissions, as well as the 2040 target for Scope 3 emissions, all of which are supported by the provided context.\n",
      "\n",
      "Score: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=google_api_key)\n",
    "reranked_docs_str = \"\\n\".join(reranked_documents)\n",
    "\n",
    "gemini_eval = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"\"\"\n",
    "                Evaluate how well the response answers the query, giving an explanation of how it answers the question, and whether the response is factually correct based on the context provided.\n",
    "                I have added the query, response and retrieved context below.\n",
    "                \n",
    "                Query: \n",
    "                {query}\n",
    "                \n",
    "                Response:\n",
    "                {response}\n",
    "                \n",
    "                Retrieved Context:\n",
    "                {reranked_docs_str}\n",
    "                \n",
    "                Give a score from 0 to 10, and a detailed explanation on the score, where:\n",
    "                - 10 = Perfectly accurate\n",
    "                - 0 = Completely incorrect\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "print(gemini_eval.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cc922-48f4-4445-ae59-692996740fde",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9e959-7632-4b44-8a33-c18ce9e49493",
   "metadata": {},
   "source": [
    "Difficulties faced and next steps: \n",
    "- I had not enough local storage space to download and extract the data of all company esg reports, so I only did a few samples. But ideally, the code should run automatically yearly to pull the latest data from all companies, and store them in a cloud storage database instead of in a local directory.\n",
    "- Deciding on the how large the chunks should be. In this assignment, I have separarated them by each sentences. However, I realised that there may be more than one sentence that could answer a query, and that some sentences may provide context for the next. Therefore, I am not sure what would be the optimal number of sentences to chunk together. If I have more time in future, I would classify each sentence, and link sentences that have the same category label sequentially together as one single chunk, for better context for the RAG application. \n",
    "- Some of the input metadata from the pdf ocr reader could not be copied because it is not permitted in PDF/A. (You may wish to examine the output PDF's XMP metadata.)\n",
    "\n",
    "Potential extensions: \n",
    "- Add more data sources, such as third party esg scoring websites and esg related news articles instead of just the company's esg reports, as the reports will be biased towards the good things that the company has done, and does not give a comprehensive view of the company's esg initiatives.\n",
    "- Integrate real-time news retrieval via APIs (Google News, Twitter), automated webscraping of the latest articles, etc. \n",
    "- Build an interactive Streamlit UI for user-friendly queries. \n",
    "\n",
    "Iterations: \n",
    "- Prompt engineering: changed the prompts in the response generation prompt multiple times to get the best results. \n",
    "- Prompt engineering: changed the query as well to be more specific, so that the retrieved documents are less ambiguous\n",
    "- Hyperparameter tuning: changed the number of documents retrieved from chromadb. If it was too many, eg 10 it may get more irrelevant documents. However, since I have split the pdf into individual sentences during chunking, there may be multiple sentences that answer the query. Therefore setting the number of documents as too low will also cause the documents retrieved to not be a comprehensive answer to the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7a05d-9d71-472e-9ac3-cf3f733aaab9",
   "metadata": {},
   "source": [
    "In the initial retriever, the document text has close semantic similarity words wise, but did not answer the question. For eg. the query was \"What are Pfizer's environmental policies\" And the result with the closest similarity was \"Updated 2022 data will be published on Pfizer's Environmental Sustainability page.\" Because the result included the words 'Pfizr' and 'environmental'. \n",
    "\n",
    "The full list of what was included is: \n",
    "1. 'As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.'\n",
    "2. 'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.'\n",
    "3. 'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.'\n",
    "4. 'Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040’.'\n",
    "5. 'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.'\n",
    "\n",
    "After being reranked, the reranked documents are \n",
    "\n",
    "1. 'Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040’.'\n",
    "2. 'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.'\n",
    "3. 'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.'\n",
    "4. 'As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.'\n",
    "5. 'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.'\n",
    "\n",
    "\n",
    "Next, we use the LLM To generate a final answer, and the generated answer answered the query much better with the document's context. It answered the question well, using the top 2 reranked documents: \n",
    "Pfizer aims to reduce its company (Scope 1 & 2) GHG emissions by 46% by 2030 from a 2019 baseline. By 2040, Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) GHG emissions and a 90% reduction in value chain (Scope 3) emissions from 2019 levels.\n",
    "\n",
    "\n",
    "This is in contrast to when no context was provided, it provided data that was not accurate. That date points \"reducing scope 2 GHG emissions by 20%\" and \"90% reduction in operational scope 1 & 2 GHG emissions\" are different from what was reported in the esg report. There was no mention of Scope 2 GHG emissions being reduced by 20%, and the reduction in scope 1&2 GHG emissions by 2040 is 95% instead of 90%). \n",
    "Here is the answer of the llm without providing context: \n",
    "Pfizer aims to reduce Scope 1 GHG emissions by 46% by 2030 from a 2019 baseline and Scope 2 GHG emissions by 20% by the same year from the same baseline. Additionally, the company aims for a 90% absolute reduction in operational Scope 1 and Scope 2 GHG emissions combined by 2040 from a 2019 baseline.\"\n",
    "\n",
    "Therefore, adding context from the vectordb helps to reduce hallucination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
