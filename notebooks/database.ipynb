{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Function to insert esg_text_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..') #go to dsa3101 folder as main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from db.scripts.db_esg_text import insert_esg_text\n",
    "df = pd.read_csv(\"./files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch prepare esg_text and batch insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_prepare_esg_text(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prepare batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        batch_data.append((\n",
    "            row['company'],\n",
    "            row['year'],\n",
    "            row['country'],\n",
    "            row['industry'],\n",
    "            row['esg_text'],\n",
    "            row['labels']\n",
    "        )) #appends a row to batch_data in tuple format for batch format\n",
    "\n",
    "        if len(batch_data) >= batch_size: #eg 100-200?\n",
    "            batches.append(batch_data)\n",
    "            batch_data = [] #reset batch\n",
    "    \n",
    "    # Append leftovers as above code doesnt account for it\n",
    "    batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_esg_text(df,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from db.scripts.db_esg_text_batch import insert_esg_text_batch\n",
    "with ProcessPoolExecutor() as executor: #allows for parallel processing\n",
    "    list(tqdm(executor.map(insert_esg_text_batch,batch), total=len(batch), desc='Insert batches into DB', unit='batch', ncols=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single ESG_Text_Insert(Small Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_esg_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into vectorDB after chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUME THIS OCCURS\n",
    "# WE STORE THE IDS, DOCUMENTS, METADATAS INTO A DB AND LOAD IT LATER TO THE CLIENT\n",
    "\n",
    "\n",
    "#  client = chromadb.PersistentClient(path=\"./chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "# collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "#     doc_text = row[\"esg_text\"]  \n",
    "#     doc_company = row[\"company\"]  \n",
    "#     doc_year = row[\"year\"]  \n",
    "#     doc_industry = row[\"industry\"]\n",
    "#     doc_id = f\"doc_{index}\"  \n",
    "\n",
    "#     collection.add(\n",
    "#         ids=[doc_id], \n",
    "#         documents=[doc_text],  \n",
    "#         metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from db.scripts.db_esg_vectorDB_batch import insert_esg_vectorDB \n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor #Parallel Processing to speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]  \n",
    "    doc_company = row[\"company\"]  \n",
    "    doc_year = row[\"year\"]  \n",
    "    doc_industry = row[\"industry\"]\n",
    "    doc_id = f\"doc_{index}\"  \n",
    "\n",
    "    insert_esg_vectorDB(\n",
    "        doc_id, \n",
    "        doc_text,\n",
    "        metadatas=json.dumps({\"company\": doc_company, \"year\": doc_year})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches:   4%|█                             | 2254/63903 [00:00<00:05, 11526.63document/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches: 100%|█████████████████████████████| 63903/63903 [00:04<00:00, 15182.37document/s]\n",
      "Inserting batches: 100%|██████████████████████████████████████| 640/640 [00:05<00:00, 107.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel batch processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def insert_esg_vectorDB_batch(batch_data):\n",
    "    # Connect to PostgreSQL (replace with your actual credentials)\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"123\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert multiple rows using executemany\n",
    "    insert_query = '''\n",
    "        INSERT INTO esg_vectorDB (doc_id, doc_text, metadatas)\n",
    "        VALUES (%s, %s, %s)\n",
    "    '''\n",
    "\n",
    "    # Use executemany to insert the batch of data\n",
    "    cur.executemany(insert_query, batch_data)\n",
    "\n",
    "    # Commit to the database\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def process_batch(batch_data):\n",
    "    \"\"\"Helper function to be used in parallel processing\"\"\"\n",
    "    insert_esg_vectorDB_batch(batch_data)\n",
    "\n",
    "# Prepare to collect documents in batches\n",
    "batch_size = 100  # Adjust this as necessary\n",
    "batch_data = []\n",
    "batches = []\n",
    "\n",
    "# Collect documents into batches\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batches\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]\n",
    "    doc_company = row[\"company\"]\n",
    "    doc_year = row[\"year\"]\n",
    "    doc_industry = row[\"industry\"]\n",
    "    doc_id = f\"doc_{index}\"\n",
    "\n",
    "    # Prepare metadata as a dictionary\n",
    "    metadatas = json.dumps({\n",
    "        \"company\": doc_company,\n",
    "        \"year\": doc_year,\n",
    "        \"industry\": doc_industry\n",
    "    })\n",
    "\n",
    "    # Append the data to the batch\n",
    "    batch_data.append((doc_id, doc_text, metadatas))\n",
    "\n",
    "    # If batch size is reached, create a batch and reset the list\n",
    "    if len(batch_data) >= batch_size:\n",
    "        batches.append(batch_data)\n",
    "        batch_data = []  # Reset for the next batch\n",
    "\n",
    "# Append any remaining data as a batch\n",
    "if batch_data:\n",
    "    batches.append(batch_data)\n",
    "\n",
    "# Parallel processing of batches\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(process_batch, batches), total=len(batches), desc=\"Inserting batches\", unit=\"batch\", ncols=100))\n",
    "\n",
    "print(\"Parallel batch processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Format of chromaDB results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'included'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = results['ids']\n",
    "doc_documents = results['documents']\n",
    "doc_metadatas = results['metadatas']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "   where={\n",
    "        \"$and\": [\n",
    "            {\"company\": \"Apple\"},\n",
    "            {\"year\": 2022.0}\n",
    "        ]\n",
    "    },\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_51558', 'doc_51413', 'doc_51407', 'doc_51406', 'doc_51420']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.',\n",
       "   'Without the methodology  change, these emissions would have increased by 14 percent, which reflects  the growth in our business.',\n",
       "   'In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.',\n",
       "   \"Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\",\n",
       "   'When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0}]],\n",
       " 'distances': [[0.9028652906417847,\n",
       "   0.9244787693023682,\n",
       "   0.9507767558097839,\n",
       "   0.9870521426200867,\n",
       "   0.9921236038208008]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
