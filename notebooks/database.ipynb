{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DB Function to insert esg_text_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..') #go to dsa3101 folder as main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from db.scripts.db_esg_text import insert_esg_text\n",
    "df = pd.read_csv(\"./files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch prepare esg_text and batch insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare batches:   0%|                                              | 0/63903 [00:00<?, ?document/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare batches: 100%|███████████████████████████████| 63903/63903 [00:03<00:00, 19185.97document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_esg_text(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prepare batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        batch_data.append((\n",
    "            row['company'],\n",
    "            int(row['year']),\n",
    "            row['country'],\n",
    "            row['industry'],\n",
    "            row['esg_text'],\n",
    "            row['labels']\n",
    "        )) #appends a row to batch_data in tuple format for batch format\n",
    "\n",
    "        if len(batch_data) >= batch_size: #eg 100-200?\n",
    "            batches.append(batch_data)\n",
    "            batch_data = [] #reset batch\n",
    "    \n",
    "    # Append leftovers as above code doesnt account for it\n",
    "    batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_esg_text(df,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert into SupaBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert batches into DB: 100%|██████████████████████████████████| 320/320 [01:31<00:00,  3.49batch/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from db.scripts.db_esg_text_batch import insert_esg_text_batch\n",
    "with ProcessPoolExecutor() as executor: #allows for parallel processing\n",
    "    list(tqdm(executor.map(insert_esg_text_batch,batch), total=len(batch), desc='Insert batches into DB', unit='batch', ncols=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single ESG_Text_Insert(Small Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_esg_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert into vectorDB in chromaDB format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUME THIS OCCURS\n",
    "# WE STORE THE IDS, DOCUMENTS, METADATAS INTO A DB AND LOAD IT LATER TO THE CLIENT\n",
    "\n",
    "\n",
    "#  client = chromadb.PersistentClient(path=\"./chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "# collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "#     doc_text = row[\"esg_text\"]  \n",
    "#     doc_company = row[\"company\"]  \n",
    "#     doc_year = row[\"year\"]  \n",
    "#     doc_industry = row[\"industry\"]\n",
    "#     doc_id = f\"doc_{index}\"  \n",
    "\n",
    "#     collection.add(\n",
    "#         ids=[doc_id], \n",
    "#         documents=[doc_text],  \n",
    "#         metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from db.scripts.db_esg_vectorDB_batch import insert_esg_vectorDB_batch\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor #Parallel Processing to speed up\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches: 100%|█████████████████████████████| 63903/63903 [00:03<00:00, 18363.03document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_chromaDB(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation, same as batch_data_prepare_esg\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        doc_text = row[\"esg_text\"]\n",
    "        doc_company = row[\"company\"]\n",
    "        doc_year = int(row[\"year\"])\n",
    "        doc_id = f\"doc_{index}\"\n",
    "\n",
    "        metadatas = json.dumps({\n",
    "            \"company\": doc_company,\n",
    "            \"year\": doc_year,\n",
    "        })\n",
    "\n",
    "        batch_data.append((doc_id, doc_text, metadatas))\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            batches.append(batch_data)\n",
    "            batch_data = []\n",
    "\n",
    "    if batch_data:\n",
    "        batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_chromaDB(df,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### insert into vectorDB in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.scripts.db_esg_vectorDB_batch import insert_esg_vectorDB_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Inserting batches: 100%|███████████████████████████████████████| 320/320 [00:03<00:00, 88.07batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(insert_esg_vectorDB_batch, batch), total=len(batch), desc=\"Inserting batches\", unit=\"batch\", ncols=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pgVector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "#Get DB Params for Local DB\n",
    "db_name = os.getenv('db_name')\n",
    "db_user = os.getenv('db_user')\n",
    "db_port = os.getenv('db_port')\n",
    "db_host = os.getenv('db_host')\n",
    "db_password = os.getenv('db_password')\n",
    "conn = psycopg2.connect(f\"dbname={db_name} user={db_user} password={db_password} host={db_host} port={db_port}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"test\",\n",
    "    connection=\"postgresql+psycopg2://postgres:123@localhost:5432/postgres\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch prepare pgVector ##Not as good as chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches: 100%|█████████████████████████████| 63903/63903 [00:03<00:00, 16506.63document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_pgVector(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation, same as batch_data_prepare_esg\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        doc_text = row[\"esg_text\"]\n",
    "        doc_company = row[\"company\"]\n",
    "        doc_year = int(row[\"year\"])\n",
    "        doc_id = index\n",
    "\n",
    "        metadatas = {\n",
    "            \"id\": doc_id,\n",
    "            \"company\": doc_company,\n",
    "            \"year\": doc_year,\n",
    "        }\n",
    "\n",
    "        batch_data.append(Document(page_content=doc_text, metadata=metadatas))\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            batches.append(batch_data)\n",
    "            batch_data = []\n",
    "\n",
    "    if batch_data:\n",
    "        batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_pgVector(df,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_vector(batch):\n",
    "    vector_store = PGVector(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=\"test\",\n",
    "        connection=\"postgresql+psycopg2://postgres:123@localhost:5432/postgres\",\n",
    "    )\n",
    "\n",
    "    vector_store.add_documents(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in batch:\n",
    "    process_batch_vector(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert batches into DB:  19%|██████▌                            | 60/320 [10:50<46:58, 10.84s/batch]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "## not faster than using CPU for processing but its faster to do this way than chromaDB which took 40mins\n",
    "with ThreadPoolExecutor() as executor: #allows for parallel processing in cpu\n",
    "    list(tqdm(executor.map(process_batch_vector,batch), total=len(batch), desc='Insert batches into DB', unit='batch', ncols=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.search(\n",
    "    query=query, \n",
    "    filter={\"company\": \"Apple\", \"year\": 2022},\n",
    "    search_type='similarity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0725c4e9-3e49-48df-a26e-c2e84bcd6b07', metadata={'id': 51401, 'year': 2022, 'company': 'Apple'}, page_content='2,780  Scope 3 (gross emissions)* 23,130,000  Business travel®  Employee commute®  Corporate carbon offsets’  Product life  cycle emissions®  (metric tons COze) Manufacturing  (purchased goods  and services)  Product transportation  (upstream and downstream)  Product use  (use of sold products)  End-of-life treatment  Product carbon offsets?'),\n",
       " Document(id='343ea235-a9ba-433a-8168-df9f555ea4b6', metadata={'id': 51420, 'year': 2022, 'company': 'Apple'}, page_content='When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.'),\n",
       " Document(id='988b4cb1-99a0-460e-824d-229241719863', metadata={'id': 51596, 'year': 2022, 'company': 'Apple'}, page_content='Scope 3 greenhouse gas  emissions related to our products, calculated Customers Communities Governance Appendix  using life cycle assessment, are checked  for quality and accuracy by the Fraunhofer  Institute in Germany in accordance with  the internationally recognized ISO 14000  environmental management standards:  ISO 14040 and 14044.'),\n",
       " Document(id='98bdd12b-c4e9-4e80-8d9e-655236b78783', metadata={'id': 51558, 'year': 2022, 'company': 'Apple'}, page_content='—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chromaDB getting from huggingFace download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa33b476f2e4209a76071cddc26b8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db34e614fc346c6bf204b4811a397bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "header.bin:   0%|          | 0.00/100 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87656950c4e34d418f71f5b9af847c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "length.bin:   0%|          | 0.00/252k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eaabad6b214969a6cb1835977608b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8578540cc4430da2effc4836919c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "link_lists.bin:   0%|          | 0.00/531k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0864a21f9d74a49a711e1adfeadb44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_level0.bin:   0%|          | 0.00/106M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032a1b3dc6f4649afc78282c7b77d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chroma.sqlite3:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eda4e30d6f4b019f0d475dbd6aab47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "index_metadata.pickle:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/shiro/dsa3101_v2/dsa3101/test'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "dataset = 'alexxtm/3101_proj_chromaDB'\n",
    "\n",
    "\n",
    "snapshot_download(local_dir=\"./test\", repo_id=dataset, repo_type='dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./chromatest\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "   where={\n",
    "        \"$and\": [\n",
    "            {\"company\": \"Apple\"},\n",
    "            {\"year\": 2022}\n",
    "        ]\n",
    "    },\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_51558', 'doc_51413', 'doc_51407', 'doc_51406', 'doc_51420']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.',\n",
       "   'Without the methodology  change, these emissions would have increased by 14 percent, which reflects  the growth in our business.',\n",
       "   'In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.',\n",
       "   \"Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\",\n",
       "   'When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0}]],\n",
       " 'distances': [[0.9028652906417847,\n",
       "   0.9244787693023682,\n",
       "   0.9507767558097839,\n",
       "   0.9870521426200867,\n",
       "   0.9921236038208008]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Wallis and Futuna</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Western Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               country   region           subregion\n",
       "0          Afghanistan     Asia       Southern Asia\n",
       "1        Åland Islands   Europe     Northern Europe\n",
       "2              Albania   Europe     Southern Europe\n",
       "3              Algeria   Africa     Northern Africa\n",
       "4       American Samoa  Oceania           Polynesia\n",
       "..                 ...      ...                 ...\n",
       "244  Wallis and Futuna  Oceania           Polynesia\n",
       "245     Western Sahara   Africa     Northern Africa\n",
       "246              Yemen     Asia        Western Asia\n",
       "247             Zambia   Africa  Sub-Saharan Africa\n",
       "248           Zimbabwe   Africa  Sub-Saharan Africa\n",
       "\n",
       "[249 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('country_regions.csv',index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
