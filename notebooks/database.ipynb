{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DB Function to insert esg_text_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..') #go to dsa3101 folder as main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from db.scripts.db_esg_text import insert_esg_text\n",
    "df = pd.read_csv(\"./files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch prepare esg_text and batch insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare batches:   0%|                                              | 0/63903 [00:00<?, ?document/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare batches: 100%|███████████████████████████████| 63903/63903 [00:03<00:00, 19185.97document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_esg_text(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prepare batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        batch_data.append((\n",
    "            row['company'],\n",
    "            int(row['year']),\n",
    "            row['country'],\n",
    "            row['industry'],\n",
    "            row['esg_text'],\n",
    "            row['labels']\n",
    "        )) #appends a row to batch_data in tuple format for batch format\n",
    "\n",
    "        if len(batch_data) >= batch_size: #eg 100-200?\n",
    "            batches.append(batch_data)\n",
    "            batch_data = [] #reset batch\n",
    "    \n",
    "    # Append leftovers as above code doesnt account for it\n",
    "    batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_esg_text(df,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert into SupaBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert batches into DB: 100%|██████████████████████████████████| 320/320 [01:31<00:00,  3.49batch/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from db.scripts.db_esg_text_batch import insert_esg_text_batch\n",
    "with ProcessPoolExecutor() as executor: #allows for parallel processing\n",
    "    list(tqdm(executor.map(insert_esg_text_batch,batch), total=len(batch), desc='Insert batches into DB', unit='batch', ncols=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single ESG_Text_Insert(Small Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_esg_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert into vectorDB in chromaDB format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUME THIS OCCURS\n",
    "# WE STORE THE IDS, DOCUMENTS, METADATAS INTO A DB AND LOAD IT LATER TO THE CLIENT\n",
    "\n",
    "\n",
    "#  client = chromadb.PersistentClient(path=\"./chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "# collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "#     doc_text = row[\"esg_text\"]  \n",
    "#     doc_company = row[\"company\"]  \n",
    "#     doc_year = row[\"year\"]  \n",
    "#     doc_industry = row[\"industry\"]\n",
    "#     doc_id = f\"doc_{index}\"  \n",
    "\n",
    "#     collection.add(\n",
    "#         ids=[doc_id], \n",
    "#         documents=[doc_text],  \n",
    "#         metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from db.scripts.db_esg_vectorDB_batch import insert_esg_vectorDB_batch\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor #Parallel Processing to speed up\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches: 100%|█████████████████████████████| 63903/63903 [00:03<00:00, 18363.03document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_chromaDB(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation, same as batch_data_prepare_esg\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        doc_text = row[\"esg_text\"]\n",
    "        doc_company = row[\"company\"]\n",
    "        doc_year = int(row[\"year\"])\n",
    "        doc_id = f\"doc_{index}\"\n",
    "\n",
    "        metadatas = json.dumps({\n",
    "            \"company\": doc_company,\n",
    "            \"year\": doc_year,\n",
    "        })\n",
    "\n",
    "        batch_data.append((doc_id, doc_text, metadatas))\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            batches.append(batch_data)\n",
    "            batch_data = []\n",
    "\n",
    "    if batch_data:\n",
    "        batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_chromaDB(df,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### insert into vectorDB in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.scripts.db_esg_vectorDB_batch import insert_esg_vectorDB_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Inserting batches: 100%|███████████████████████████████████████| 320/320 [00:03<00:00, 88.07batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(insert_esg_vectorDB_batch, batch), total=len(batch), desc=\"Inserting batches\", unit=\"batch\", ncols=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pgVector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "#Get DB Params for Local DB\n",
    "db_name = os.getenv('db_name')\n",
    "db_user = os.getenv('db_user')\n",
    "db_port = os.getenv('db_port')\n",
    "db_host = os.getenv('db_host')\n",
    "db_password = os.getenv('db_password')\n",
    "conn = psycopg2.connect(f\"dbname={db_name} user={db_user} password={db_password} host={db_host} port={db_port}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"test\",\n",
    "    connection=\"postgresql+psycopg2://postgres:123@localhost:5432/postgres\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch prepare pgVector ##Not as good as chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing batches: 100%|█████████████████████████████| 63903/63903 [00:03<00:00, 16506.63document/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_data_prepare_pgVector(df, batch_size):\n",
    "    batch_data = [] #batch of data to append\n",
    "    batches = [] #index of batches\n",
    "    \n",
    "    #batch data_preparation, same as batch_data_prepare_esg\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batches\", unit=\"document\", leave=True, ncols=100):\n",
    "        doc_text = row[\"esg_text\"]\n",
    "        doc_company = row[\"company\"]\n",
    "        doc_year = int(row[\"year\"])\n",
    "        doc_id = index\n",
    "\n",
    "        metadatas = {\n",
    "            \"id\": doc_id,\n",
    "            \"company\": doc_company,\n",
    "            \"year\": doc_year,\n",
    "        }\n",
    "\n",
    "        batch_data.append(Document(page_content=doc_text, metadata=metadatas))\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            batches.append(batch_data)\n",
    "            batch_data = []\n",
    "\n",
    "    if batch_data:\n",
    "        batches.append(batch_data)\n",
    "    return batches\n",
    "\n",
    "batch = batch_data_prepare_pgVector(df,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_vector(batch):\n",
    "    vector_store = PGVector(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=\"test\",\n",
    "        connection=\"postgresql+psycopg2://postgres:123@localhost:5432/postgres\",\n",
    "    )\n",
    "\n",
    "    vector_store.add_documents(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in batch:\n",
    "    process_batch_vector(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert batches into DB:  19%|██████▌                            | 60/320 [10:50<46:58, 10.84s/batch]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "## not faster than using CPU for processing but its faster to do this way than chromaDB which took 40mins\n",
    "with ThreadPoolExecutor() as executor: #allows for parallel processing in cpu\n",
    "    list(tqdm(executor.map(process_batch_vector,batch), total=len(batch), desc='Insert batches into DB', unit='batch', ncols=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.search(\n",
    "    query=query, \n",
    "    filter={\"company\": \"Apple\", \"year\": 2022},\n",
    "    search_type='similarity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0725c4e9-3e49-48df-a26e-c2e84bcd6b07', metadata={'id': 51401, 'year': 2022, 'company': 'Apple'}, page_content='2,780  Scope 3 (gross emissions)* 23,130,000  Business travel®  Employee commute®  Corporate carbon offsets’  Product life  cycle emissions®  (metric tons COze) Manufacturing  (purchased goods  and services)  Product transportation  (upstream and downstream)  Product use  (use of sold products)  End-of-life treatment  Product carbon offsets?'),\n",
       " Document(id='343ea235-a9ba-433a-8168-df9f555ea4b6', metadata={'id': 51420, 'year': 2022, 'company': 'Apple'}, page_content='When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.'),\n",
       " Document(id='988b4cb1-99a0-460e-824d-229241719863', metadata={'id': 51596, 'year': 2022, 'company': 'Apple'}, page_content='Scope 3 greenhouse gas  emissions related to our products, calculated Customers Communities Governance Appendix  using life cycle assessment, are checked  for quality and accuracy by the Fraunhofer  Institute in Germany in accordance with  the internationally recognized ISO 14000  environmental management standards:  ISO 14040 and 14044.'),\n",
       " Document(id='98bdd12b-c4e9-4e80-8d9e-655236b78783', metadata={'id': 51558, 'year': 2022, 'company': 'Apple'}, page_content='—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chromaDB getting from huggingFace download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa33b476f2e4209a76071cddc26b8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db34e614fc346c6bf204b4811a397bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "header.bin:   0%|          | 0.00/100 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87656950c4e34d418f71f5b9af847c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "length.bin:   0%|          | 0.00/252k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eaabad6b214969a6cb1835977608b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8578540cc4430da2effc4836919c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "link_lists.bin:   0%|          | 0.00/531k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0864a21f9d74a49a711e1adfeadb44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_level0.bin:   0%|          | 0.00/106M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032a1b3dc6f4649afc78282c7b77d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chroma.sqlite3:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eda4e30d6f4b019f0d475dbd6aab47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "index_metadata.pickle:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/shiro/dsa3101_v2/dsa3101/test'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "dataset = 'alexxtm/3101_proj_chromaDB'\n",
    "\n",
    "\n",
    "snapshot_download(local_dir=\"./test\", repo_id=dataset, repo_type='dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./chromatest\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "   where={\n",
    "        \"$and\": [\n",
    "            {\"company\": \"Apple\"},\n",
    "            {\"year\": 2022}\n",
    "        ]\n",
    "    },\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_51558', 'doc_51413', 'doc_51407', 'doc_51406', 'doc_51420']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.',\n",
       "   'Without the methodology  change, these emissions would have increased by 14 percent, which reflects  the growth in our business.',\n",
       "   'In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.',\n",
       "   \"Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\",\n",
       "   'When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0}]],\n",
       " 'distances': [[0.9028652906417847,\n",
       "   0.9244787693023682,\n",
       "   0.9507767558097839,\n",
       "   0.9870521426200867,\n",
       "   0.9921236038208008]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to create db_exec values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "### if u got strong gpu w cuda, should change to gpu, average laptop cpu takes too long *cough* mac book users\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"running on cuda\")\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "google_api_key = \"AIzaSyCutzQsZEOJUQgHwcvjtPNiLFbgyxOfmko\"\n",
    "from openai import OpenAI\n",
    "API_KEY = \"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\" \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"esg_metrics.json\", \"r\") as file:\n",
    "    esg_metrics = json.load(file)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb_1003\")\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "df_columns = [\"Company\"] + list(esg_metrics.keys())  # One column per ESG metric\n",
    "df_metrics = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Function to retrieve relevant ESG text using ChromaDB\n",
    "def retrieve_esg_text(company, query):\n",
    "    collection = chroma_client.get_collection(name=\"esgtext\")\n",
    "    results = collection.query(query_texts=[query], n_results=5)\n",
    "    return results\n",
    "\n",
    "# Function to rerank retrieved documents\n",
    "def get_reranked_docs(query, results):\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "    return reranked_docs\n",
    "    \n",
    "# Function to extract metric values using DeepSeek  \n",
    "def extract_values(query, retrieved_text):\n",
    "    reranked_docs = get_reranked_docs(query, results)\n",
    "    response = generate_response(query, reranked_docs)\n",
    "    return response[\"text\"]  ######## Adjust based on DeepSeek output format\n",
    "\n",
    "# Function to compute the score based on thresholds\n",
    "def compute_linear_score(extracted_values, thresholds):\n",
    "    ####\n",
    "    pass\n",
    "\n",
    "# List of companies\n",
    "companies = [\"Pfizer\", \"Apple\", \"Datadog\"]  # Replace with actual company list\n",
    "\n",
    "# Process each company\n",
    "for company in companies:\n",
    "    row_data = {\"Company\": company}\n",
    "\n",
    "    for metric, details in esg_metrics.items():\n",
    "        query = details[\"query\"]\n",
    "        scoring_thresholds = details[\"scoring_thresholds\"]\n",
    "\n",
    "        # Retrieve ESG text using ChromaDB\n",
    "        retrieved_text = retrieve_esg_text(company, query)\n",
    "\n",
    "        # Extract values using DeepSeek\n",
    "        extracted_values = extract_values(query, retrieved_text)\n",
    "\n",
    "        # Compute score\n",
    "        score = compute_score(extracted_values, scoring_thresholds)\n",
    "\n",
    "        # Store results\n",
    "        row_data[metric] = {\"extracted_values\": extracted_values, \"score\": score}\n",
    "\n",
    "    # Append to DataFrame\n",
    "    df_metrics = df_metricsdf.append(row_data, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_metrics.to_csv(\"company_esg_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
