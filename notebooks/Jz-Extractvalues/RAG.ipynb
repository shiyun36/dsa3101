{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b38a7-2950-4936-9ffa-025243c15b21",
   "metadata": {},
   "source": [
    "# Scoring with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5e659-63fd-4355-a5ad-b5c38160fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "### if u got strong gpu w cuda, should change to gpu, average laptop cpu takes too long *cough* mac book users\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"running on cuda\")\n",
    "\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6022ba-6ae2-4953-a77c-2671fca48332",
   "metadata": {},
   "source": [
    "# RAG testing ignore for move into py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73636d00-c3d8-4f2b-8a48-5b53c736ab73",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../files/labeled_pdfs_1003.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../files/labeled_pdfs_1003.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jz\\.conda\\envs\\3101_proj\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../files/labeled_pdfs_1003.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb2f2f",
   "metadata": {},
   "source": [
    "Adding into DB -> shift into PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8abda4-601d-40e9-8e35-4e119b5b8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to load ../files/labeled_pdfs_1003.csv: [Errno 2] No such file or directory: '../files/labeled_pdfs_1003.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def company_year_exists(company, year):\n",
    "    \"\"\"\n",
    "    Check if a document with the given company and year already exists in the collection.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[\"\"],  # Empty query text since we're filtering solely by metadata\n",
    "        n_results=1,\n",
    "        where={\n",
    "    \"$and\": [\n",
    "        {\"company\": company},\n",
    "        {\"year\": float(year)}\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "    )\n",
    "    # Check if any document was returned\n",
    "    if results.get(\"documents\") and results[\"documents\"][0]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def add_documents_from_csv(file_path):\n",
    "    client = chromadb.PersistentClient(path=\"../chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "    collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "    \"\"\"\n",
    "    Add documents from a CSV file to the collection.\n",
    "    Assumes CSV has columns: 'esg_text', 'company', 'year', 'industry'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Capture the starting document count to keep doc_ids consistent\n",
    "    groups = df.groupby([\"company\", \"year\"])\n",
    "\n",
    "    # Process each group only once\n",
    "    for (company, year), group_df in tqdm(groups, total=len(groups), desc=\"Processing groups\", unit=\"group\", ncols=100):\n",
    "        if company_year_exists(company, year):\n",
    "            print(f\"Group for {company} ({year}) already exists. Skipping all documents for this group.\")\n",
    "            continue\n",
    "\n",
    "        # Capture starting count for unique doc_ids for this group\n",
    "        starting_count = collection.count()\n",
    "        # Add all rows in the group\n",
    "        for i, (_, row) in enumerate(group_df.iterrows()):\n",
    "            doc_text = row[\"esg_text\"]\n",
    "            doc_company = row[\"company\"]\n",
    "            doc_year = row[\"year\"]\n",
    "            doc_industry = row[\"industry\"]\n",
    "            doc_country = row[\"country\"]\n",
    "\n",
    "            doc_id = f\"doc_{starting_count + i}\"\n",
    "            print(f\"Adding document {doc_id} for {doc_company} ({doc_year})\")\n",
    "            collection.add(\n",
    "                ids=[doc_id],\n",
    "                documents=[doc_text],\n",
    "                metadatas=[{\n",
    "                    \"company\": doc_company, \n",
    "                    \"year\": doc_year, \n",
    "                    \"industry\": doc_industry,\n",
    "                    \"country\": doc_country\n",
    "                }]\n",
    "            )\n",
    "\n",
    "add_documents_from_csv(\"../files/labeled_pdfs_1003.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5dadc0f-2077-4cf3-81ed-e63b76e2b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71212\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\"../chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "\n",
    "print(collection.count())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5956bd",
   "metadata": {},
   "source": [
    "Ignore this below, its just to check if its working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb4a89d-dbfe-4790-b740-f6ceba5234b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'Total Greenhouse Gas Emissions': {'value_query': \"Extract the total carbon emissions in tcO2e for the year 2023. This may be presented as 'Total Scope 1 and 2 Emissions'. Please output the extracted values in the specified structure seen in the key-value pair for extracted_values below and try not to leave any blanks. Write a brief description that summarizes the extracted values.\", 'extracted_values': {'total_emissions': '', 'description': ''}, 'scoring_query': 'Based on the values extracted from value_query, give a score based on this criteria: 1 - (total_emissions/100000). Return the final_score as a float from 0 to 1. If total_emissions cannot be found, return the final score as 0.5', 'final_score': ''}}\n",
      "Extract the total carbon emissions in tcO2e for the year 2023. This may be presented as 'Total Scope 1 and 2 Emissions'. Please output the extracted values in the specified structure seen in the key-value pair for extracted_values below and try not to leave any blanks. Write a brief description that summarizes the extracted values.\n",
      "{'ids': [['doc_59168', 'doc_59170', 'doc_58514', 'doc_58747', 'doc_59166', 'doc_58506', 'doc_58504', 'doc_58540', 'doc_58484', 'doc_59067', 'doc_58501', 'doc_59059', 'doc_58539', 'doc_59169', 'doc_59179']], 'embeddings': None, 'documents': [['purchased cooling?: * Generation of Scope 2 emissions (location-based): 70.215 thousand tCO,e * Generation of Scope 2 emissions (market-based): 1.583 thousand tCO,e 3 Other indirect (Scope 3) ¢ Indirect emissions from domestic and international air Group emissions?', '2 tCO,e refers to the tonnes of carbon dioxide equivalent.', '86.8 te, eo 84.4 82.7 2.8 oP tenon, Ornccccessveececcee® 22.7 ceveeesses@eren Beorere 82.0 71.6 79.1 89.1 1,754 2,153 2,585 4.0 0.6 2018 2021 2022 2023 2018 2021 2022 2023 HM Scope 1 (Stationary combustion, refrigerant, etc.) M™ Scope 2 (Electricity purchase) ® Scope 3 (Business air travel) + Scope 2 GHG emissions intensity excluding data centres -- Scope 2 GHG emissions intensity including data centres Note: Scope 1 and Scope 2 emissions data for 2021 and 2022 were restated due to re-categorisation of steam emissions from Scope 1 to Scope 2.', 'Total 1.6 1.1 1.1 68.8 71.3 Scope 3 non-financed*4 Air travel*° 14.1 4.0 0.6 1.5 11.6 Total Scope 1, Scope 2 market-based and Scope 3 20.5 10.9 6.0 70.3 82.9 Carbon offsets retired 20.5 10.9 6.0 - - GHG emissions intensity (kg CO,e per m?', 'stationary combustion emissions, fugitive emissions, mobile combustion emissions): * Generation of Scope 1 emissions: 4.830 thousand tCO,e?', 'GHG Scope 1, 2 and 3 emissions (thousand tCO,e) and GHG emissions intensity (kgCO,e/m?) Waste recycled and disposed Our data for 2023 cover all branches and offices in our key markets, including estimates from sites where we could not obtain actual data.', 'Our Scope 3 emissions increased from 4.0 thousand tCO,e to 14.1 thousand tCO,e due to increased regional business travel.', 'Scope 2 emissions are the main component of our operational emissions profile, arising from our purchase of grid electricity and outsourced data centres.', \"In 2023, we purchased and retired 20,479 carbon offsets to address our Scope 1, residual Scope 2' and Scope 3 (business air travel) emissions.\", '8 305-2 Energy indirect a) Gross location-based energy indirect (Scope 2) GHG Group (Scope 2) GHG emissions in metric tons of CO, equivalent.', 'including data centres UOB SUSTAINABILITY REPORT 2023 91 GHG emissions Our overall Scope 2 emissions intensity (measured in kilograms (kg) of CO,-equivalent per m?', 'd) Base year for the calculation, if applicable, including: i) the rationale for choosing it; iil) emissions in the base year; ili) the context for any significant changes in emissions that triggered recalculations of base year emissions.', 'Scope 1 and Scope 2 emissions are calculated based on location-based GHG conversion factors from international standards by the United Kingdom Department for Environment, Food and Rural Affairs and the International Energy Agency respectively.', '© travel undertaken by employees for business purposes from, to or within Singapore, Indonesia, Malaysia, Thailand, Vietnam, mainland China, Hong Kong and South Korea: * Generation of Scope 3 emissions: 14.066 thousand tCO,e 1 Data includes all UOB divisions and banking subsidiaries, excluding asset/investment management subsidiaries.', \"in I W UOB SUSTAINABILITY REPORT 2023 163 No Greenhouse Gas Emissions Coverage Scope 4 Total GHG emissions * Total Scope 1, Scope 2 (location-based), Scope 3 Group emissions: 89.111 thousand tCO.,e * Total Scope 1, Scope 2 (market-based), Scope 3 emissions: 20.479 thousand tCO,e 5 Carbon credits’ * Amount of carbon credits retired: 20.479 thousandtCO,e —- UOB Management's responsibilities UOB Management is responsible for selecting the Criteria, and for presenting the Subject Matter in accordance with that Criteria, in all material respects.\"]], 'uris': None, 'data': None, 'metadatas': [[{'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}, {'company': 'UOBGROUP', 'country': 'Singapore', 'industry': 'Finance', 'year': 2023}]], 'distances': [[0.7688901424407959, 0.8047834038734436, 0.8157261610031128, 0.8437663912773132, 0.8501080274581909, 0.8528010249137878, 0.8658117651939392, 0.9234397411346436, 0.9718013405799866, 0.9766262769699097, 0.981046736240387, 0.9908624887466431, 0.9913023710250854, 0.9927130937576294, 0.9979031085968018]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "print(company_year_exists(\"UOBGROUP\",2023.0))\n",
    "client = chromadb.PersistentClient(path=\"../chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_collection(name=\"dsa3101\")\n",
    "\n",
    "with open(\"../../files/scoring_queries/sg_bank_query.json\", \"r\") as file:\n",
    "    esg_metrics = json.load(file)\n",
    "\n",
    "metric_name = esg_metrics[0]\n",
    "print(metric_name)  \n",
    "query = esg_metrics[0]['Total Greenhouse Gas Emissions']['value_query']\n",
    "print(query)\n",
    "\n",
    "def retrieve_esg_text(company, query):\n",
    "\n",
    "    results = collection.query(query_texts=[query], n_results=15,where={\"$and\": [{\"company\": company[0]}, {\"year\": company[1]}]})\n",
    "\n",
    "    if not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        print(f\"Company '{company[0]}' of '{company[1]}' is not in database, stopping code and throwing error...\")\n",
    "        sys.exit(1)\n",
    "    return results\n",
    "\n",
    "company_info =   (\"UOBGROUP\", 2023.0)\n",
    "\n",
    "print(retrieve_esg_text(company_info, query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d927e8-1983-44ab-89d2-b039e98ac88f",
   "metadata": {},
   "source": [
    "## Testing the Generator\n",
    "Setting up LLM API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb83d1f-32ca-484d-a33e-693229822f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import httpx\n",
    "genai.configure(api_key=google_api_key)\n",
    "llm_genai = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "llm_openai = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=API_KEY,\n",
    "    http_client= httpx.Client()\n",
    ")\n",
    "#llm_deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "\n",
    "reranker_model_name = \"BAAI/bge-reranker-base\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "reranker_model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d27a395b-1e9d-40d1-afea-357647f78eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_genai_response(query, reranked_docs):\n",
    "\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context + \"End of Context\"}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include 'ids' next to it, like this. *information* (\"doc_xxxxx\")\n",
    "    \"\"\"\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "          completion = llm_genai.generate_content(prompt)\n",
    "\n",
    "          if completion and completion.text:\n",
    "                return {\"text\": completion.text.strip()}\n",
    "          else:\n",
    "                print(\"Empty completion received. Retrying...\")\n",
    "                retries -= 1\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"API Error encountered: {e}. Retrying after delay...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"API Error: Unable to generate response after retries.\"\n",
    "\n",
    "def generate_openai_response(query, reranked_docs):\n",
    "    print(\"Chatgpttt\")\n",
    "\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context + \"End of Context\"}\n",
    "\n",
    "    \"\"\"\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = llm_openai.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",  # or your preferred model\n",
    "                messages=[\n",
    "                    {\"role\": \"assistant\", \"content\": \"You are an expert in ESG analysis looking through several documents\"},\n",
    "                    { \"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            if response and response.choices:\n",
    "                return {\"text\": response.choices[0].message.content.strip()}\n",
    "            else:\n",
    "                print(\"Empty completion received. Retrying...\")\n",
    "                retries -= 1\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"API Error encountered: {e}. Retrying after delay...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "    return \"API Error: Unable to generate response after retries.\"\n",
    "\n",
    "'''def generate_deepseek_response(query, reranked_docs):\n",
    "    print(\"deepseeking\")\n",
    "\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context + \"End of Context\"}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include 'ids' next to it, like this. *information* (\"doc_xxxxx\")\n",
    "    \"\"\"\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = llm_deepseek.completions.create(\n",
    "                engine=\"deepseek/deepseek-r1-zero:free\",  # or your preferred model\n",
    "                messages=[\n",
    "                    { \"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            if response['choices'][0] and response['choices'][0]['text']:\n",
    "                return {\"text\": response['choices'][0]['text']}\n",
    "            else:\n",
    "                print(\"Empty completion received. Retrying...\")\n",
    "                retries -= 1\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"API Error encountered: {e}. Retrying after delay...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "    return \"API Error: Unable to generate response after retries.\"'''\n",
    "\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved documents based on relevance scores using the BAAI/bge-reranker-base model.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        retrieved_docs (list): A list of retrieved document texts.\n",
    "\n",
    "    Returns:\n",
    "        list: The reranked documents sorted by relevance.\n",
    "    \"\"\"\n",
    "    if not retrieved_docs:\n",
    "        return []\n",
    "\n",
    "    # tokenise the query? = solve the ranking\n",
    "    query_inputs = reranker_tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_outputs = reranker_model(**query_inputs,output_hidden_states=True)\n",
    "    # For simplicity, use the [CLS] token (first token) as the query embedding.\n",
    "    query_embedding = query_outputs.hidden_states[-1][:, 0]\n",
    "    \n",
    "    doc_inputs = reranker_tokenizer(retrieved_docs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        doc_outputs = reranker_model(**doc_inputs,output_hidden_states=True)\n",
    "    # Use the [CLS] token embedding for each document.\n",
    "    doc_embeddings = doc_outputs.hidden_states[-1][:, 0]\n",
    "\n",
    "    # Compute relevance scores\n",
    "    similarities = F.cosine_similarity(query_embedding, doc_embeddings, dim=-1)  # shape: (num_docs,)\n",
    "\n",
    "    # Sort retrieved docs by relevance score (descending order)\n",
    "    sorted_indices = similarities.argsort(descending=True)\n",
    "    reranked_docs = [retrieved_docs[i] for i in sorted_indices.tolist()]\n",
    "    return reranked_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b0fc0-c719-4026-9524-1b6dbd144372",
   "metadata": {},
   "source": [
    "# Start of full RAG code with json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb9ee38b-56d3-44f6-9440-72040051b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_company_metadata(company_tuple):\n",
    "    \"\"\"\n",
    "    Query the collection to retrieve metadata for a given company.\n",
    "    Expects company_tuple to be (company_name, year).\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"../chromadb_1003\")\n",
    "    collection = chroma_client.get_or_create_collection(name=\"dsa3101\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"\"],  # no text query; we use metadata filtering only\n",
    "        n_results=1,\n",
    "        where={\"$and\": [{\"company\": company_tuple[0]}, {\"year\": company_tuple[1]}]}\n",
    "    )\n",
    "    if not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        print(f\"Company '{company_tuple[0]}' for year '{company_tuple[1]}' is not in the database. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Assumes metadata is stored along with the document. For example:\n",
    "    # metadatas: [[{\"company\": \"DBS\", \"year\": 2023.0, \"country\": \"Singapore\", \"industry\": \"Finance\"}]]\n",
    "    metadata = results[\"metadatas\"][0][0]\n",
    "    if \"country\" not in metadata or \"industry\" not in metadata:\n",
    "        print(f\"Metadata for {company_tuple[0]} is missing 'country' or 'industry' fields. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    return metadata\n",
    "\n",
    "def retrieve_esg_text(company_tuple, query):\n",
    "    \"\"\"\n",
    "    Retrieve ESG text from ChromaDB for a company given a specific query.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=15,\n",
    "        where={\"$and\": [{\"company\": company_tuple[0]}, {\"year\": company_tuple[1]}]}\n",
    "    )\n",
    "    if not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        print(f\"Company '{company_tuple[0]}' for year '{company_tuple[1]}' is not in the database. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    return results\n",
    "\n",
    "def get_reranked_docs(query, results):\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "    return reranked_docs\n",
    "\n",
    "def extract_values(query, results):\n",
    "    reranked_docs = get_reranked_docs(query, results)\n",
    "    response = generate_openai_response(query, reranked_docs)\n",
    "    return response\n",
    "\n",
    "def compute_linear_score(extracted_values, scoring_query):\n",
    "    final_answer_generator = 'Return your answer as: \"Final Answer: X\" (where X is a numeric or clearly defined answer)'\n",
    "    score = generate_openai_response(scoring_query + final_answer_generator, str(extracted_values))\n",
    "    return score\n",
    "\n",
    "def extract_absolute_score(score):\n",
    "    matches = re.findall(r\"(?:Final\\s*Answer|Answer).*?([-+]?\\d*\\.?\\d+)(?:\\s*\\%\\.)?\", str(score), re.DOTALL)\n",
    "    return float(matches[-1]) if matches else \"N/A\"\n",
    "\n",
    "def process_company(company_tuple):\n",
    "\n",
    "    metadata = retrieve_company_metadata(company_tuple)\n",
    "    country = metadata.get(\"country\", \"\").strip().lower()\n",
    "    industry = metadata.get(\"industry\", \"\").strip().lower()\n",
    "    \n",
    "    # Choose JSON query file and CSV file based on country and industry.\n",
    "    if country == \"singapore\":\n",
    "        if industry == \"finance\":\n",
    "            query_file = \"../../files/scoring_queries/sg_bank_query.json\"\n",
    "            csv_file = \"sg_finance_score.csv\"\n",
    "        elif industry == \"health\":\n",
    "            query_file = \"../../files/scoring_queries/sg_healthcare_query.json\"\n",
    "            csv_file = \"sg_healthcare_score.csv\"\n",
    "        else:\n",
    "            print(f\"Unsupported industry '{industry}' for country '{country}'. Exiting.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Unsupported country '{country}'. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load the ESG metrics from the JSON file\n",
    "    try:\n",
    "        with open(query_file, \"r\") as file:\n",
    "            esg_metrics = json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file '{query_file}': {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Prepare DataFrame headers based on the metrics in the JSON file.\n",
    "    headers = [list(item.keys())[0] for item in esg_metrics]\n",
    "    df_columns = [\"Company\", \"Year\"] + headers  # one column per ESG metric\n",
    "    \n",
    "    # Load existing CSV for this combination if it exists\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "        existing_companies = set(map(tuple, df_existing[['Company', 'Year']].values.tolist()))\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=df_columns)\n",
    "        existing_companies = set()\n",
    "        \n",
    "    # Skip if company has already been processed for this CSV\n",
    "    if tuple(company_tuple) in existing_companies:\n",
    "        print(f\"Company '{company_tuple[0]}' for year '{company_tuple[1]}' already processed; skipping.\")\n",
    "        return None, csv_file, df_columns\n",
    "\n",
    "    row_data = {\"Company\": company_tuple[0], \"Year\": company_tuple[1]}\n",
    "    \n",
    "    # Process each ESG metric defined in the JSON file.\n",
    "    for metric_item in esg_metrics:\n",
    "        for metric, details in metric_item.items():\n",
    "            metric_name = metric\n",
    "            query = details[\"value_query\"]\n",
    "            scoring_thresholds = details[\"scoring_query\"]\n",
    "\n",
    "            # Retrieve ESG text using the query.\n",
    "            retrieved_text = retrieve_esg_text(company_tuple, query)\n",
    "            # Extract values using DeepSeek (or your equivalent method).\n",
    "            extracted_values = extract_values(query, retrieved_text)\n",
    "            # Compute the linear score.\n",
    "            score = compute_linear_score(extracted_values, scoring_thresholds)\n",
    "            \n",
    "            row_data[metric_name] = {\"extracted_values\": extracted_values, \"score\": score}\n",
    "    \n",
    "    return row_data, csv_file, df_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use a dictionary to group new rows by the target CSV file.\n",
    "def extract_esgreports(companies):\n",
    "    \n",
    "\n",
    "    new_rows = {}\n",
    "\n",
    "    for company_tuple in companies:\n",
    "        # process_company is assumed to be defined elsewhere\n",
    "        row_data, csv_file, df_columns = process_company(company_tuple)\n",
    "\n",
    "        # If no row_data, skip\n",
    "        if row_data is None:\n",
    "            continue\n",
    "\n",
    "        # If this CSV hasn't been seen yet, initialize a new entry\n",
    "        if csv_file not in new_rows:\n",
    "            new_rows[csv_file] = {\"rows\": [], \"columns\": df_columns}\n",
    "\n",
    "        # Add the row data to our in-memory collection\n",
    "        new_rows[csv_file][\"rows\"].append(row_data)\n",
    "\n",
    "    # Once all rows are collected, write or append them to their respective CSVs\n",
    "    for csv_file, data in new_rows.items():\n",
    "        df_new = pd.DataFrame(data[\"rows\"], columns=data[\"columns\"])\n",
    "\n",
    "        if os.path.exists(csv_file):\n",
    "            df_existing = pd.read_csv(csv_file)\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            df_combined = df_new\n",
    "\n",
    "        df_combined.to_csv(csv_file, index=False)\n",
    "        print(f\"Added {len(data['rows'])} new companies to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ddf6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company 'DBS' for year '2023.0' already processed; skipping.\n",
      "Company 'UOBGROUP' for year '2023.0' already processed; skipping.\n",
      "Company 'OCBC' for year '2023.0' already processed; skipping.\n",
      "Company 'ECONHEALTHCARE' for year '2024.0' already processed; skipping.\n",
      "Company 'FULLERTONHEALTH' for year '2023.0' already processed; skipping.\n",
      "Company 'HEALTHMEDICAL' for year '2022.0' already processed; skipping.\n",
      "Company 'HSA' for year '2023.0' already processed; skipping.\n"
     ]
    }
   ],
   "source": [
    "# List of companies to process (each as a tuple: (Company, Year))\n",
    "companies = [\n",
    "    (\"DBS\", 2023.0),\n",
    "    (\"UOBGROUP\", 2023.0),\n",
    "    (\"OCBC\", 2023.0),\n",
    "    (\"ECONHEALTHCARE\", 2024.0),\n",
    "    (\"FULLERTONHEALTH\", 2023.0),\n",
    "    (\"HEALTHMEDICAL\", 2022.0),\n",
    "    (\"HSA\", 2023.0)\n",
    "]\n",
    "\n",
    "extract_esgreports(companies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a18b68",
   "metadata": {},
   "source": [
    "### SubIndusry Score database\n",
    "Extract the scores from the sub_industry csv\n",
    "\n",
    "1. Parse the extracted values to get final answer, then return final score with metric into json saved as a subindustry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bc86d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. JSON file saved as: sg_healthcare_score.json\n",
      "Conversion complete. JSON file saved as: sg_finance_score.json\n"
     ]
    }
   ],
   "source": [
    "def extract_to_json(subindustry_filepath):\n",
    "    csv_file = subindustry_filepath\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    company_col = df.columns[0]\n",
    "    year_col = df.columns[1]\n",
    "    other_cols = df.columns[2:]  \n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        company = row[company_col]\n",
    "        year = row[year_col]\n",
    "        details = {}\n",
    "    \n",
    "        for col in other_cols:\n",
    "            value = row[col]\n",
    "            extracted = extract_absolute_score(value)\n",
    "            details[col] = extracted if extracted is not None else value\n",
    "\n",
    "        if company not in data_dict:\n",
    "            data_dict[company] = {}\n",
    "            data_dict[company][year] = details\n",
    "\n",
    "    json_file = os.path.splitext(csv_file)[0] + \".json\"\n",
    "\n",
    "    with open(json_file, \"w\") as f:\n",
    "      json.dump(data_dict, f, indent=4)\n",
    "\n",
    "    print(\"Conversion complete. JSON file saved as:\", json_file)\n",
    "\n",
    "extract_to_json('sg_healthcare_score.csv')\n",
    "extract_to_json('sg_finance_score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c99e0-fb4a-4186-b21c-f152da2d2c11",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "When you store documents in ChromaDB using collection.add(), it:\n",
    "\n",
    "1. Generates vector embeddings for your text (if you haven't provided them).\n",
    "2. Stores the document along with its embedding in the vector database.\n",
    "3. Matches queries based on similarity search (cosine similarity by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f764-ef3f-4011-9561-ec60198ce604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655d54a7-83f2-4638-aa9f-2fb069a0f738",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "Checking for hallucination, irrelevance, bias \n",
    "In this assignment, I felt that biasness wasn't really a metric required, I think it would be good to add biasness if i extracted data from third party sources grading the company esg scores. I can then compare the third-party metrics and scoring to each company's esg reports, and check if there is biasness in terms of their ratings, towards a particular, company or industry, etc. Therefore, I just added the metric for future reference, but it is not required in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047809e-b41f-4a4f-a583-94f731705305",
   "metadata": {},
   "source": [
    "### Hallucination detection (Faithfullness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e5914-e633-44c0-996d-1a3471006ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def fuzzy_match(sentence, doc, threshold=80):\n",
    "    \"\"\"Check if sentence has a fuzzy match in the document.\"\"\"\n",
    "    return fuzz.partial_ratio(normalize_text(sentence), normalize_text(doc)) >= threshold\n",
    "\n",
    "def verify_facts(response, reranked_docs, fuzzy_threshold=80):\n",
    "    \"\"\"Detect hallucinations by checking if sentences exist in retrieved docs using fuzzy matching.\"\"\"\n",
    "    missing_facts = []\n",
    "    \n",
    "    # Split response into sentences and check if they appear in any of the documents\n",
    "    for sent in response.split(\". \"):\n",
    "        found = any(fuzzy_match(sent, doc, fuzzy_threshold) for doc in reranked_docs)\n",
    "        if not found:\n",
    "            missing_facts.append(sent)\n",
    "\n",
    "    if missing_facts:\n",
    "        print(\"Warning: Some statements are not found in the retrieved context:\")\n",
    "        for fact in missing_facts:\n",
    "            print(f\"- {fact}\")\n",
    "    \n",
    "    return 1 - len(missing_facts) / len(response.split(\". \"))  # Faithfulness Score\n",
    "\n",
    "faithfulness_score = verify_facts(response, reranked_documents)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e383-f6c2-4755-86c6-db9fcfc00bc5",
   "metadata": {},
   "source": [
    "## Irrelevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda28345-5526-41e5-8d04-1e3cb97f9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def check_relevance(query, response, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Check the relevance of the response to the query using semantic similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    response_embedding = model.encode([response])\n",
    "\n",
    "    similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        return True, similarity  \n",
    "    else:\n",
    "        return False, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c8dd-2b62-4e30-bbe9-51973dacffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_relevant = check_relevance(query, response)\n",
    "print(f\"Is the response relevant? {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c597bf1-8a94-4e40-99da-74907eaa51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-reranker-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def check_bias(text):\n",
    "    \"\"\"\n",
    "    Check for potential bias in the text using a pretrained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Assuming binary classification (0 = no bias, 1 = biased)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class == 1  # 1 indicates bias (this depends on the model's labeling)\n",
    "\n",
    "# Example Usage\n",
    "response = \"Pfizer has been focusing on improving diversity in their clinical trials and sharing their insights with others as part of their diversity and inclusion initiatives in 2022.\"\n",
    "\n",
    "is_biased = check_bias(response)\n",
    "print(f\"Is the response biased? {is_biased}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb7396-1af0-423d-8676-a48c8516eb68",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa267f2-b321-4758-afac-a20d44832149",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "Typical metrics: RecalL@k, Precision @k, Mean Reciprocal Rank, Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc47e47-9939-43d9-8c8d-e0130e7be26f",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5510ac0-4b18-4606-a1b4-b155d3634525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(query, reranked_docs):\n",
    "    \"\"\"Compute semantic similarity between query and retrieved docs.\"\"\"\n",
    "    corpus = [query] + reranked_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    return similarity_scores.mean()  # Average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71078-d266-46bf-9106-943f2dd30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(query, reranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f70ad-88b8-4692-a72c-775ba45db831",
   "metadata": {},
   "source": [
    "## Generator Evaluation \n",
    "Typical metrics: ROUGE, BLEU, BERTScore, domain-specific or task-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410181f7-bc69-415c-991c-5461bdb5b27b",
   "metadata": {},
   "source": [
    "### BLEU Score (Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9b46e-b114-427a-a366-d5c22324a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(reference, generated_response):\n",
    "    \"\"\"Compare generated response against reference text using BLEU score.\"\"\"\n",
    "    reference_tokens = reference.lower().split()\n",
    "    generated_tokens = generated_response.lower().split()\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5aae-8086-4306-b318-d697e93cccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_bleu_score(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb8f94-3737-4fcf-b003-74a17bfa839f",
   "metadata": {},
   "source": [
    "### Retrieval score (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6208a-09cd-4eb8-9523-b78e3152a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(reranked_docs, response):\n",
    "    \"\"\"Calculate how relevant the response is to the retrieved documents.\"\"\"\n",
    "    corpus = reranked_docs + [response]  # Combine all docs and response\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    return similarity_matrix.mean()  # Average similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c430da-47b0-464a-a510-d5a3ac0c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(reranked_docs, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4a61-3080-4ed3-929f-6560fe92648a",
   "metadata": {},
   "source": [
    "### Judge LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3e25-334e-4c48-82f4-d9a27b677207",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=google_api_key)\n",
    "reranked_docs_str = \"\\n\".join(reranked_documents)\n",
    "\n",
    "gemini_eval = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"\"\"\n",
    "                Evaluate how well the response answers the query, giving an explanation of how it answers the question, and whether the response is factually correct based on the context provided.\n",
    "                I have added the query, response and retrieved context below.\n",
    "                \n",
    "                Query: \n",
    "                {query}\n",
    "                \n",
    "                Response:\n",
    "                {response}\n",
    "                \n",
    "                Retrieved Context:\n",
    "                {reranked_docs_str}\n",
    "                \n",
    "                Give a score from 0 to 10, and a detailed explanation on the score, where:\n",
    "                - 10 = Perfectly accurate\n",
    "                - 0 = Completely incorrect\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "print(gemini_eval.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
