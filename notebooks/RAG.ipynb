{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b38a7-2950-4936-9ffa-025243c15b21",
   "metadata": {},
   "source": [
    "# Scoring with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1fd5e659-63fd-4355-a5ad-b5c38160fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "### if u got strong gpu w cuda, should change to gpu, average laptop cpu takes too long *cough* mac book users\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"running on cuda\")\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "google_api_key = \"AIzaSyCutzQsZEOJUQgHwcvjtPNiLFbgyxOfmko\"\n",
    "from openai import OpenAI\n",
    "API_KEY = \"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\" \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6022ba-6ae2-4953-a77c-2671fca48332",
   "metadata": {},
   "source": [
    "# RAG testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73636d00-c3d8-4f2b-8a48-5b53c736ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8abda4-601d-40e9-8e35-4e119b5b8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|█████████████████████████████████| 63903/63903 [31:58<00:00, 33.32document/s]\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]  \n",
    "    doc_company = row[\"company\"]  \n",
    "    doc_year = row[\"year\"]  \n",
    "    doc_industry = row[\"industry\"]\n",
    "    doc_id = f\"doc_{index}\"  \n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id], \n",
    "        documents=[doc_text],  \n",
    "        metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5dadc0f-2077-4cf3-81ed-e63b76e2b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63903\n"
     ]
    }
   ],
   "source": [
    "print(collection.count())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8eb4a89d-dbfe-4790-b740-f6ceba5234b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_51558', 'doc_51413', 'doc_51407', 'doc_51406', 'doc_51420']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['—> Continue reading on page 13  Reduced overall  emissions by 40%  In fiscal year 2021, our environmental  initiatives avoided over 23 million metric  tons of emissions across all scopes, and  we reduced our carbon footprint by  40 percent compared with fiscal year  2015.',\n",
       "   'Without the methodology  change, these emissions would have increased by 14 percent, which reflects  the growth in our business.',\n",
       "   'In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.',\n",
       "   \"Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\",\n",
       "   'When using the  same level of data granularity and model as 2021, our product use carbon  emissions in 2021 would have been about 2.5 percent lower.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0},\n",
       "   {'company': 'Apple', 'year': 2022.0}]],\n",
       " 'distances': [[0.9028652906417847,\n",
       "   0.9244787693023682,\n",
       "   0.9507767558097839,\n",
       "   0.9870521426200867,\n",
       "   0.9921236038208008]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d927e8-1983-44ab-89d2-b039e98ac88f",
   "metadata": {},
   "source": [
    "## Testing the Generator\n",
    "Use DeepSeek API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "abb83d1f-32ca-484d-a33e-693229822f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import httpx\n",
    "genai.configure(api_key=google_api_key)\n",
    "llm_genai = genai.GenerativeModel('gemini-2.0-flash')\n",
    "llmopenai= OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=API_KEY,\n",
    "    http_client=httpx.Client()\n",
    ")\n",
    "reranker_model_name = \"BAAI/bge-reranker-base\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "reranker_model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d27a395b-1e9d-40d1-afea-357647f78eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, reranked_docs):\n",
    "\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "          completion = llm_genai.generate_content(prompt)\n",
    "\n",
    "          if completion and completion.text:\n",
    "                return {\"text\": completion.text.strip()}\n",
    "          else:\n",
    "                print(\"Empty completion received. Retrying...\")\n",
    "                retries -= 1\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"API Error encountered: {e}. Retrying after delay...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"API Error: Unable to generate response after retries.\"\n",
    "\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved documents based on relevance scores using the BAAI/bge-reranker-base model.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        retrieved_docs (list): A list of retrieved document texts.\n",
    "\n",
    "    Returns:\n",
    "        list: The reranked documents sorted by relevance.\n",
    "    \"\"\"\n",
    "    if not retrieved_docs:\n",
    "        return []\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = reranker_tokenizer(\n",
    "        [query] + retrieved_docs,  \n",
    "        padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute relevance scores\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "    # Sort retrieved docs by relevance score (descending order)\n",
    "    reranked_docs = [doc for _, doc in sorted(zip(scores[1:], retrieved_docs), reverse=True)]\n",
    "\n",
    "    return reranked_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b0fc0-c719-4026-9524-1b6dbd144372",
   "metadata": {},
   "source": [
    "# Start of full RAG code with json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb9ee38b-56d3-44f6-9440-72040051b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
      "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
      "    \n",
      "    Question: Retrieve the percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in 3 types: a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction. Please output the extracted values in the specified structure seen in the key-value pair for extracted_values below and do not leave any blanks. The description summarizes all the extracted values in a concise statement.\n",
      "\n",
      "    Context:\n",
      "    Total gross carbon footprint (without offsets)\"®  (metric tons COze)  Total net carbon footprint (after applying offsets)”  (metric tons COze) 22,850  85,570  -167,000  16,200,000  1,750,000  4,990,000  80,000  -500,000  23,200,000  22,530,000 2020  47,430  39,340  4,270  3,830  22,550,000  153,000  134,000  -70,000  16,100,000  1,800,000  4,300,000  60,000  22,600,000  22,530,000 Fiscal year  52,730  40,910  6,950  4,870  24,980,000  326,000  195,000  18,900,000  1,400,000  4,100,000  60,000  25,100,000  25,100,000 2018  57,440  42,840  11,110  3,490  8,730  8,730  25,070,000  337,000  183,000  18,500,000  1,300,000  4,700,000  50,000  25,200,000  25,200,000 2017  47,050  36,210  8,300  2,540  36,250  36,250  27,330,000  121,000  172,000  21,100,000  1,200,000  4,700,000  40,000  27,500,000  27,500,000 10.\n",
      "\n",
      "EFFICIENCY:  0.06 million metric  , 0.2 million metric tons avoided MODE SWITCHING:  tons avoided 0.2 million metric  tons avoided 0.2 million metric tons avoided  PRODUCT ENERGY EFFICIENCY: LOAD REDUCTION AND  Gross emissions  Offsets i  EERREEERES mmm 7777777722 2 re tetrttttettt  Emissions categories 0.02% 0% 0.5% 70% 22% 8% 0.3%  (% of gross emissions) Direct emissions Electricity Business travel Product Product use Product transport End-of-life product  (Scope 1) (Scope 2) and commute manufacturing (Scope 3) (Scope 3) processing  (Scope 3) (Scope 3) (Scope 3)  * Low-carbon materials represents emissions savings from transitioning to recycled materials in our products, or use of low-carbon aluminum, as described on page 18.\n",
      "\n",
      "Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\n",
      "\n",
      "In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.\n",
      "\n",
      "Percentages shown for each emissions category represent the share of Apple’s gross footprint.\n",
      "\n",
      "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
      "    \n",
      "You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
      "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
      "    \n",
      "    Question: Obtain score by taking the average of the percentages and dividing the average by 50. If the resulting score is greater than 1, fix score as 1\n",
      "\n",
      "    Context:\n",
      "    text\n",
      "\n",
      "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
      "    \n",
      "query: Retrieve the percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in 3 types: a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction. Please output the extracted values in the specified structure seen in the key-value pair for extracted_values below and do not leave any blanks. The description summarizes all the extracted values in a concise statement.\n",
      "scoring thresh:Obtain score by taking the average of the percentages and dividing the average by 50. If the resulting score is greater than 1, fix score as 1\n",
      "retrieved:{'ids': [['doc_51407', 'doc_51402', 'doc_51456', 'doc_51458', 'doc_51406']], 'embeddings': None, 'documents': [['In fiscal year 2017, we started calculating scope 3 emissions not listed in  this table.', 'Total gross carbon footprint (without offsets)\"®  (metric tons COze)  Total net carbon footprint (after applying offsets)”  (metric tons COze) 22,850  85,570  -167,000  16,200,000  1,750,000  4,990,000  80,000  -500,000  23,200,000  22,530,000 2020  47,430  39,340  4,270  3,830  22,550,000  153,000  134,000  -70,000  16,100,000  1,800,000  4,300,000  60,000  22,600,000  22,530,000 Fiscal year  52,730  40,910  6,950  4,870  24,980,000  326,000  195,000  18,900,000  1,400,000  4,100,000  60,000  25,100,000  25,100,000 2018  57,440  42,840  11,110  3,490  8,730  8,730  25,070,000  337,000  183,000  18,500,000  1,300,000  4,700,000  50,000  25,200,000  25,200,000 2017  47,050  36,210  8,300  2,540  36,250  36,250  27,330,000  121,000  172,000  21,100,000  1,200,000  4,700,000  40,000  27,500,000  27,500,000 10.', 'EFFICIENCY:  0.06 million metric  , 0.2 million metric tons avoided MODE SWITCHING:  tons avoided 0.2 million metric  tons avoided 0.2 million metric tons avoided  PRODUCT ENERGY EFFICIENCY: LOAD REDUCTION AND  Gross emissions  Offsets i  EERREEERES mmm 7777777722 2 re tetrttttettt  Emissions categories 0.02% 0% 0.5% 70% 22% 8% 0.3%  (% of gross emissions) Direct emissions Electricity Business travel Product Product use Product transport End-of-life product  (Scope 1) (Scope 2) and commute manufacturing (Scope 3) (Scope 3) processing  (Scope 3) (Scope 3) (Scope 3)  * Low-carbon materials represents emissions savings from transitioning to recycled materials in our products, or use of low-carbon aluminum, as described on page 18.', 'Percentages shown for each emissions category represent the share of Apple’s gross footprint.', \"Beginning in FY2021, we're accounting for scope 2 emissions from the  purchase of district heating, chilled water, and steam.\"]], 'uris': None, 'data': None, 'metadatas': [[{'company': 'Apple', 'year': 2022.0}, {'company': 'Apple', 'year': 2022.0}, {'company': 'Apple', 'year': 2022.0}, {'company': 'Apple', 'year': 2022.0}, {'company': 'Apple', 'year': 2022.0}]], 'distances': [[1.045174479484558, 1.0579395294189453, 1.065542459487915, 1.0941659212112427, 1.116015076637268]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n",
      "scoring test:{'text': '```json\\n{\\n\"total_reduction\": \"No values found\",\\n\"scope_1_reduction\": \"No values found\",\\n\"scope_2_reduction\": \"No values found\"\\n}\\n```\\nDescription: The document does not contain the percentage of reduction in Greenhouse gas emissions during the reporting year, either in total, for Scope 1, or for Scope 2.'}\n",
      "score: {'text': 'I need the percentages to calculate the average and then divide by 50 to obtain the score. Since the provided text is empty, I cannot perform the calculation.'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../files/Scoring_revised.json\", \"r\") as file:\n",
    "    esg_metrics = json.load(file)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb_1003\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"dsa3101\")\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "df_columns = [\"Company\"] + [list(metric.keys())[0] for metric in esg_metrics]  # One column per ESG metric\n",
    "df_metrics = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Function to retrieve relevant ESG text using ChromaDB\n",
    "def retrieve_esg_text(company, query):\n",
    "    collection = chroma_client.get_collection(name=\"dsa3101\")\n",
    "    results = collection.query(query_texts=[query], n_results=5,where={\"company\": company})\n",
    "    return results\n",
    "\n",
    "# Function to rerank retrieved documents\n",
    "def get_reranked_docs(query, results):\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "    return reranked_docs\n",
    "    \n",
    "# Function to extract metric values using DeepSeek  \n",
    "def extract_values(query, results):\n",
    "    reranked_docs = get_reranked_docs(query, results)\n",
    "    response = generate_response(query, reranked_docs)\n",
    "    return response  \n",
    "# Function to compute the score based on thresholds\n",
    "def compute_linear_score(extracted_values, scoring_query):\n",
    "    score = generate_response(scoring_query,extracted_values)\n",
    "    return score\n",
    "    \n",
    "\n",
    "# List of companies\n",
    "companies = [\"Apple\"]  # Replace with actual company list\n",
    "\n",
    "\n",
    "# Process each company\n",
    "for company in companies:\n",
    "    row_data = {\"Company\": company}\n",
    "\n",
    "    #for metric_dict in esg_metrics:\n",
    "metric_name = list(esg_metrics[0].keys())[0]  # Get the metric name\n",
    "details = esg_metrics[0][metric_name]\n",
    "query = details[\"query\"]\n",
    "scoring_thresholds = details[\"scoring_query\"]\n",
    "\n",
    "        # Retrieve ESG text using ChromaDB\n",
    "retrieved_text = retrieve_esg_text(company, query)\n",
    "\n",
    "        # Extract values using DeepSeek\n",
    "extracted_values = extract_values(query, retrieved_text)\n",
    "\n",
    "        # Compute score\n",
    "score = compute_linear_score(extracted_values, scoring_thresholds)\n",
    "\n",
    "        # Store results\n",
    "row_data[metric_name] = {\"extracted_values\": extracted_values, \"score\": score}\n",
    "\n",
    "    # Append to DataFrame\n",
    "print(\"query: \"+ query)\n",
    "print(\"scoring thresh:\"+ scoring_thresholds)\n",
    "print(\"retrieved:\" + str(retrieved_text))\n",
    "print(\"scoring test:\" + str(extracted_values))\n",
    "print(\"score: \"+ str(score))\n",
    "#df_metrics = df_metrics.append(row_data, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "#df_metrics.to_csv(\"company_esg_scores.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c99e0-fb4a-4186-b21c-f152da2d2c11",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "When you store documents in ChromaDB using collection.add(), it:\n",
    "\n",
    "1. Generates vector embeddings for your text (if you haven't provided them).\n",
    "2. Stores the document along with its embedding in the vector database.\n",
    "3. Matches queries based on similarity search (cosine similarity by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f764-ef3f-4011-9561-ec60198ce604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655d54a7-83f2-4638-aa9f-2fb069a0f738",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "Checking for hallucination, irrelevance, bias \n",
    "In this assignment, I felt that biasness wasn't really a metric required, I think it would be good to add biasness if i extracted data from third party sources grading the company esg scores. I can then compare the third-party metrics and scoring to each company's esg reports, and check if there is biasness in terms of their ratings, towards a particular, company or industry, etc. Therefore, I just added the metric for future reference, but it is not required in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047809e-b41f-4a4f-a583-94f731705305",
   "metadata": {},
   "source": [
    "### Hallucination detection (Faithfullness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e5914-e633-44c0-996d-1a3471006ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def fuzzy_match(sentence, doc, threshold=80):\n",
    "    \"\"\"Check if sentence has a fuzzy match in the document.\"\"\"\n",
    "    return fuzz.partial_ratio(normalize_text(sentence), normalize_text(doc)) >= threshold\n",
    "\n",
    "def verify_facts(response, reranked_docs, fuzzy_threshold=80):\n",
    "    \"\"\"Detect hallucinations by checking if sentences exist in retrieved docs using fuzzy matching.\"\"\"\n",
    "    missing_facts = []\n",
    "    \n",
    "    # Split response into sentences and check if they appear in any of the documents\n",
    "    for sent in response.split(\". \"):\n",
    "        found = any(fuzzy_match(sent, doc, fuzzy_threshold) for doc in reranked_docs)\n",
    "        if not found:\n",
    "            missing_facts.append(sent)\n",
    "\n",
    "    if missing_facts:\n",
    "        print(\"Warning: Some statements are not found in the retrieved context:\")\n",
    "        for fact in missing_facts:\n",
    "            print(f\"- {fact}\")\n",
    "    \n",
    "    return 1 - len(missing_facts) / len(response.split(\". \"))  # Faithfulness Score\n",
    "\n",
    "faithfulness_score = verify_facts(response, reranked_documents)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e383-f6c2-4755-86c6-db9fcfc00bc5",
   "metadata": {},
   "source": [
    "## Irrelevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda28345-5526-41e5-8d04-1e3cb97f9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def check_relevance(query, response, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Check the relevance of the response to the query using semantic similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    response_embedding = model.encode([response])\n",
    "\n",
    "    similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        return True, similarity  \n",
    "    else:\n",
    "        return False, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c8dd-2b62-4e30-bbe9-51973dacffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_relevant = check_relevance(query, response)\n",
    "print(f\"Is the response relevant? {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c597bf1-8a94-4e40-99da-74907eaa51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-reranker-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def check_bias(text):\n",
    "    \"\"\"\n",
    "    Check for potential bias in the text using a pretrained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Assuming binary classification (0 = no bias, 1 = biased)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class == 1  # 1 indicates bias (this depends on the model's labeling)\n",
    "\n",
    "# Example Usage\n",
    "response = \"Pfizer has been focusing on improving diversity in their clinical trials and sharing their insights with others as part of their diversity and inclusion initiatives in 2022.\"\n",
    "\n",
    "is_biased = check_bias(response)\n",
    "print(f\"Is the response biased? {is_biased}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb7396-1af0-423d-8676-a48c8516eb68",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa267f2-b321-4758-afac-a20d44832149",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "Typical metrics: RecalL@k, Precision @k, Mean Reciprocal Rank, Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc47e47-9939-43d9-8c8d-e0130e7be26f",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5510ac0-4b18-4606-a1b4-b155d3634525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(query, reranked_docs):\n",
    "    \"\"\"Compute semantic similarity between query and retrieved docs.\"\"\"\n",
    "    corpus = [query] + reranked_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    return similarity_scores.mean()  # Average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71078-d266-46bf-9106-943f2dd30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(query, reranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f70ad-88b8-4692-a72c-775ba45db831",
   "metadata": {},
   "source": [
    "## Generator Evaluation \n",
    "Typical metrics: ROUGE, BLEU, BERTScore, domain-specific or task-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410181f7-bc69-415c-991c-5461bdb5b27b",
   "metadata": {},
   "source": [
    "### BLEU Score (Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9b46e-b114-427a-a366-d5c22324a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(reference, generated_response):\n",
    "    \"\"\"Compare generated response against reference text using BLEU score.\"\"\"\n",
    "    reference_tokens = reference.lower().split()\n",
    "    generated_tokens = generated_response.lower().split()\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5aae-8086-4306-b318-d697e93cccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_bleu_score(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb8f94-3737-4fcf-b003-74a17bfa839f",
   "metadata": {},
   "source": [
    "### Retrieval score (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6208a-09cd-4eb8-9523-b78e3152a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(reranked_docs, response):\n",
    "    \"\"\"Calculate how relevant the response is to the retrieved documents.\"\"\"\n",
    "    corpus = reranked_docs + [response]  # Combine all docs and response\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    return similarity_matrix.mean()  # Average similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c430da-47b0-464a-a510-d5a3ac0c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(reranked_docs, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4a61-3080-4ed3-929f-6560fe92648a",
   "metadata": {},
   "source": [
    "### Judge LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3e25-334e-4c48-82f4-d9a27b677207",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=google_api_key)\n",
    "reranked_docs_str = \"\\n\".join(reranked_documents)\n",
    "\n",
    "gemini_eval = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"\"\"\n",
    "                Evaluate how well the response answers the query, giving an explanation of how it answers the question, and whether the response is factually correct based on the context provided.\n",
    "                I have added the query, response and retrieved context below.\n",
    "                \n",
    "                Query: \n",
    "                {query}\n",
    "                \n",
    "                Response:\n",
    "                {response}\n",
    "                \n",
    "                Retrieved Context:\n",
    "                {reranked_docs_str}\n",
    "                \n",
    "                Give a score from 0 to 10, and a detailed explanation on the score, where:\n",
    "                - 10 = Perfectly accurate\n",
    "                - 0 = Completely incorrect\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "print(gemini_eval.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
