{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b38a7-2950-4936-9ffa-025243c15b21",
   "metadata": {},
   "source": [
    "# ESG financial Insights Generator \n",
    "\n",
    "The system will retrieve the latest ESG reports and regulatory documents to generate concicse, insightful summaries answering ESG-related queries for a specific ompany. \n",
    "\n",
    "Problem statement: How can a tool summarise ESG related news of each company? \n",
    "\n",
    "Background: \n",
    "Based on these newly sourced company ESG reports, and the the previous classification method from assignment1, I can classify these chunks of text into the E,S and G categories, so that model is able to better generate a response related to one of the particular categories. For example, the query could be \"What are the recent carbon emission goals of Pfizer?\" Since in the data loading portion of this assignmnet I have added Pfizer's ESG report that details ESG strategies, and \"carbon emission\" is under the \"environmental\" category, based on these contexts, the model is able to get the specific document chunk context and metadata (the categories) to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd5e659-63fd-4355-a5ad-b5c38160fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "torch.set_default_device(\"cpu\")\n",
    "import random\n",
    "import json\n",
    "from google import genai\n",
    "google_api_key = \"AIzaSyCutzQsZEOJUQgHwcvjtPNiLFbgyxOfmko\"\n",
    "from openai import OpenAI\n",
    "API_KEY = \"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\" \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6022ba-6ae2-4953-a77c-2671fca48332",
   "metadata": {},
   "source": [
    "# RAG Pipeline Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c99e0-fb4a-4186-b21c-f152da2d2c11",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "When you store documents in ChromaDB using collection.add(), it:\n",
    "\n",
    "1. Generates vector embeddings for your text (if you haven't provided them).\n",
    "2. Stores the document along with its embedding in the vector database.\n",
    "3. Matches queries based on similarity search (cosine similarity by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73636d00-c3d8-4f2b-8a48-5b53c736ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8abda4-601d-40e9-8e35-4e119b5b8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa4265_ass2\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]  \n",
    "    doc_company = row[\"Company\"]  \n",
    "    doc_year = row[\"Year\"]  \n",
    "    doc_id = f\"doc_{index}\"  \n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id], \n",
    "        documents=[doc_text],  \n",
    "        metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dadc0f-2077-4cf3-81ed-e63b76e2b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection.count())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b911e81-79af-4529-a8f3-ecfbc0e1ec24",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "Method: Semantic similarity to compare embeddings of the query to the sentences, and retrieve the top sentences with the highest similarity scores.\n",
    "Limitations: Different words may have different meanings under different contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c60d83-b651-4376-8f65-98f0f14c1c91",
   "metadata": {},
   "source": [
    "### Retrieval & Query Handling\n",
    "\n",
    "Hybrid Search pipeline\n",
    "\n",
    "Step 1: Vector Search in ChromaDB â†’ Retrieve top-k relevant documents based on semantic similarity.\n",
    "\n",
    "Step 2: BGE Reranker Search -> Evaluates how relevant the retrieved documents are by comparing each document with the query, scoring them, and reordering them according to relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41199cad-f303-40cf-bbdf-8a90e8ddd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"dsa4265_ass2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8642cd89-d3ef-4160-981d-9ca5a5c7a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the carbon emission goals of pfizer\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    where={\"company\": \"Pfizer\"},\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e68400-2765-4e09-85d5-3eeaef3fe868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_10665', 'doc_10666', 'doc_9646', 'doc_9655', 'doc_9648']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.',\n",
       "   'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.',\n",
       "   'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5Â°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.',\n",
       "   'Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040â€™.',\n",
       "   'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022},\n",
       "   {'company': 'Pfizer', 'year': 2022}]],\n",
       " 'distances': [[0.5328716039657593,\n",
       "   0.5343808531761169,\n",
       "   0.5423866510391235,\n",
       "   0.5645685195922852,\n",
       "   0.6181094646453857]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788fa6f-c0bf-4c0d-bc1c-429fc4ff4659",
   "metadata": {},
   "source": [
    "### Reranking\n",
    "Rerank documents using bge-reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "799274ff-1f69-44de-a767-9445d6fb91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Docs: ['Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) greenhouse gas (GHG) emissions and a 90% reduction in value chain  (Scope 3) emissions by 2040â€™.', 'In 2020, after achieving our third generation GHG emission  reduction goal, Pfizer set a new goal to reduce Scope 1 & 2  GHG emissions 46% by 2030 from a 2019 baseline and, by  2025, advancing goals to reduce emissions in three Scope  3 categories.', 'Pfizer is continuing its near-term commitment to reduce  company Greenhouse Gas (GHG) emissions aligned witha  1.5Â°C trajectory and to engage suppliers so that they also  set science-based GHG emissions reduction goals.', 'As part of the  commitment, Pfizer aims to decrease our GHG emissions by  95% and value chain emissions by 90% from 2019 levels by  2040 through accelerating the transition away from fossil  fuels and engaging suppliers to catalyze equivalent action.', 'By 2040 Pfizer  aims to decrease its company GHG emissions by 95% and  its value chain emissions by 90% from 2019 levels through  accelerating the transition away from fossil fuels and  engaging suppliers to catalyze equivalent action.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "model_name = \"BAAI/bge-reranker-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [query] + retrieved_docs,  \n",
    "    padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Compute relevance scores\n",
    "with torch.no_grad():\n",
    "    scores = model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "# Sort docs by relevance score\n",
    "reranked_docs = [doc for _, doc in sorted(zip(scores[1:], retrieved_docs), reverse=True)]\n",
    "print(\"Reranked Docs:\", reranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d927e8-1983-44ab-89d2-b039e98ac88f",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Use DeepSeek API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abb83d1f-32ca-484d-a33e-693229822f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "643b776a-11c2-4abc-9259-5e6403ece55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, reranked_docs):\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1-zero:free\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in ESG analysis. Answer factually and ensure consistency with the provided context, especially focusing on environemntal, sustainability and governance principles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if completion and completion.choices and completion.choices[0].message:\n",
    "                return completion.choices[0].message.content  # Return model's response\n",
    "\n",
    "            print(\"Warning: Empty response. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"Error: Unable to generate a response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25449c11-be13-40b3-ad53-82a46f9e4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Generated Answer:\n",
      " \\boxed{Pfizer aims to reduce its company (Scope 1 & 2) GHG emissions by 46% by 2030 from a 2019 baseline. By 2040, Pfizer aims to achieve a 95% reduction in company (Scope 1 & 2) GHG emissions and a 90% reduction in value chain (Scope 3) emissions from 2019 levels.}\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(query, reranked_docs)\n",
    "print(\"\\nðŸ”¹ Generated Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da8a7d-aaf9-47bc-aac1-b76dda656a22",
   "metadata": {},
   "source": [
    "### Generating a response from Deepseek without the retrieval step's context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79869882-f27a-47b4-afd9-a9424231d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_without_context(query):\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a factually accurate response. \n",
    "    \"\"\"\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1-zero:free\", #deepseek-reasoner #deepseek/deepseek-r1:free\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in ESG analysis. Answer factually and ensure consistency with the provided context, especially focusing on environemntal, sustainability and governance principles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if completion and completion.choices and completion.choices[0].message:\n",
    "                return completion.choices[0].message.content  \n",
    "\n",
    "            print(\"Warning: Empty response. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"Error: Unable to generate a response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c928cc91-7c8b-4c69-a20d-daee1d1e95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\boxed{\\n\"Pfizer aims to reduce Scope 1 GHG emissions by 46% by 2030 from a 2019 baseline and Scope 2 GHG emissions by 20% by the same year from the same baseline. Additionally, the company aims for a 90% absolute reduction in operational Scope 1 and Scope 2 GHG emissions combined by 2040 from a 2019 baseline.\"\\n}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_without_context(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d54a7-83f2-4638-aa9f-2fb069a0f738",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "Checking for hallucination, irrelevance, bias \n",
    "In this assignment, I felt that biasness wasn't really a metric required, I think it would be good to add biasness if i extracted data from third party sources grading the company esg scores. I can then compare the third-party metrics and scoring to each company's esg reports, and check if there is biasness in terms of their ratings, towards a particular, company or industry, etc. Therefore, I just added the metric for future reference, but it is not required in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047809e-b41f-4a4f-a583-94f731705305",
   "metadata": {},
   "source": [
    "### Hallucination detection (Faithfullness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7e5914-e633-44c0-996d-1a3471006ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def fuzzy_match(sentence, doc, threshold=80):\n",
    "    \"\"\"Check if sentence has a fuzzy match in the document.\"\"\"\n",
    "    return fuzz.partial_ratio(normalize_text(sentence), normalize_text(doc)) >= threshold\n",
    "\n",
    "def verify_facts(response, reranked_docs, fuzzy_threshold=80):\n",
    "    \"\"\"Detect hallucinations by checking if sentences exist in retrieved docs using fuzzy matching.\"\"\"\n",
    "    missing_facts = []\n",
    "    \n",
    "    # Split response into sentences and check if they appear in any of the documents\n",
    "    for sent in response.split(\". \"):\n",
    "        found = any(fuzzy_match(sent, doc, fuzzy_threshold) for doc in reranked_docs)\n",
    "        if not found:\n",
    "            missing_facts.append(sent)\n",
    "\n",
    "    if missing_facts:\n",
    "        print(\"Warning: Some statements are not found in the retrieved context:\")\n",
    "        for fact in missing_facts:\n",
    "            print(f\"- {fact}\")\n",
    "    \n",
    "    return 1 - len(missing_facts) / len(response.split(\". \"))  # Faithfulness Score\n",
    "\n",
    "faithfulness_score = verify_facts(response, reranked_documents)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e383-f6c2-4755-86c6-db9fcfc00bc5",
   "metadata": {},
   "source": [
    "## Irrelevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda28345-5526-41e5-8d04-1e3cb97f9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def check_relevance(query, response, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Check the relevance of the response to the query using semantic similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    response_embedding = model.encode([response])\n",
    "\n",
    "    similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        return True, similarity  \n",
    "    else:\n",
    "        return False, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0730c8dd-2b62-4e30-bbe9-51973dacffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the response relevant? (True, 0.6938479)\n"
     ]
    }
   ],
   "source": [
    "is_relevant = check_relevance(query, response)\n",
    "print(f\"Is the response relevant? {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c597bf1-8a94-4e40-99da-74907eaa51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the response biased? False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-reranker-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def check_bias(text):\n",
    "    \"\"\"\n",
    "    Check for potential bias in the text using a pretrained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Assuming binary classification (0 = no bias, 1 = biased)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class == 1  # 1 indicates bias (this depends on the model's labeling)\n",
    "\n",
    "# Example Usage\n",
    "response = \"Pfizer has been focusing on improving diversity in their clinical trials and sharing their insights with others as part of their diversity and inclusion initiatives in 2022.\"\n",
    "\n",
    "is_biased = check_bias(response)\n",
    "print(f\"Is the response biased? {is_biased}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb7396-1af0-423d-8676-a48c8516eb68",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa267f2-b321-4758-afac-a20d44832149",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "Typical metrics: RecalL@k, Precision @k, Mean Reciprocal Rank, Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc47e47-9939-43d9-8c8d-e0130e7be26f",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5510ac0-4b18-4606-a1b4-b155d3634525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(query, reranked_docs):\n",
    "    \"\"\"Compute semantic similarity between query and retrieved docs.\"\"\"\n",
    "    corpus = [query] + reranked_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    return similarity_scores.mean()  # Average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea71078-d266-46bf-9106-943f2dd30a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09264488847701834"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_retrieval_relevance(query, reranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f70ad-88b8-4692-a72c-775ba45db831",
   "metadata": {},
   "source": [
    "## Generator Evaluation \n",
    "Typical metrics: ROUGE, BLEU, BERTScore, domain-specific or task-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410181f7-bc69-415c-991c-5461bdb5b27b",
   "metadata": {},
   "source": [
    "### BLEU Score (Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5f9b46e-b114-427a-a366-d5c22324a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(reference, generated_response):\n",
    "    \"\"\"Compare generated response against reference text using BLEU score.\"\"\"\n",
    "    reference_tokens = reference.lower().split()\n",
    "    generated_tokens = generated_response.lower().split()\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3c5aae-8086-4306-b318-d697e93cccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/chienshiyun/miniconda3/envs/4265_assignment1/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.50440384721771e-232"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu_score(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb8f94-3737-4fcf-b003-74a17bfa839f",
   "metadata": {},
   "source": [
    "### Retrieval score (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6208a-09cd-4eb8-9523-b78e3152a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(reranked_docs, response):\n",
    "    \"\"\"Calculate how relevant the response is to the retrieved documents.\"\"\"\n",
    "    corpus = reranked_docs + [response]  # Combine all docs and response\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    return similarity_matrix.mean()  # Average similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c430da-47b0-464a-a510-d5a3ac0c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(reranked_docs, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4a61-3080-4ed3-929f-6560fe92648a",
   "metadata": {},
   "source": [
    "### Judge LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aab3e25-334e-4c48-82f4-d9a27b677207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response accurately and completely answers the query about Pfizer's carbon emission goals. It provides both the 2030 and 2040 targets for Scope 1 & 2 emissions, as well as the 2040 target for Scope 3 emissions, all of which are supported by the provided context.\n",
      "\n",
      "Score: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=google_api_key)\n",
    "reranked_docs_str = \"\\n\".join(reranked_documents)\n",
    "\n",
    "gemini_eval = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"\"\"\n",
    "                Evaluate how well the response answers the query, giving an explanation of how it answers the question, and whether the response is factually correct based on the context provided.\n",
    "                I have added the query, response and retrieved context below.\n",
    "                \n",
    "                Query: \n",
    "                {query}\n",
    "                \n",
    "                Response:\n",
    "                {response}\n",
    "                \n",
    "                Retrieved Context:\n",
    "                {reranked_docs_str}\n",
    "                \n",
    "                Give a score from 0 to 10, and a detailed explanation on the score, where:\n",
    "                - 10 = Perfectly accurate\n",
    "                - 0 = Completely incorrect\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "print(gemini_eval.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(4265_assignment1)",
   "language": "python",
   "name": "4265_assignment1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
