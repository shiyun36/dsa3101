{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b38a7-2950-4936-9ffa-025243c15b21",
   "metadata": {},
   "source": [
    "# Scoring with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd5e659-63fd-4355-a5ad-b5c38160fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import chromadb  # Vector Database\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import AdamW  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "### if u got strong gpu w cuda, should change to gpu, average laptop cpu takes too long *cough* mac book users\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"running on cuda\")\n",
    "import random\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "google_api_key = \"AIzaSyCutzQsZEOJUQgHwcvjtPNiLFbgyxOfmko\"\n",
    "from openai import OpenAI\n",
    "API_KEY = \"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\" \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from googlesearch import search\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6022ba-6ae2-4953-a77c-2671fca48332",
   "metadata": {},
   "source": [
    "# RAG testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c99e0-fb4a-4186-b21c-f152da2d2c11",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "When you store documents in ChromaDB using collection.add(), it:\n",
    "\n",
    "1. Generates vector embeddings for your text (if you haven't provided them).\n",
    "2. Stores the document along with its embedding in the vector database.\n",
    "3. Matches queries based on similarity search (cosine similarity by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73636d00-c3d8-4f2b-8a48-5b53c736ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../files/labeled_pdfs_1003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8abda4-601d-40e9-8e35-4e119b5b8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents:   0%|                                  | 27/63903 [00:03<2:10:12,  8.18document/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m doc_industry \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m---> 12\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompany\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_company\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_year\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/models/Collection.py:82\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     49\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     add_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_add(\n\u001b[1;32m     92\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     93\u001b[0m         ids\u001b[38;5;241m=\u001b[39madd_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py:213\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_add_request\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_records[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     validate_record_set_for_embedding(record_set\u001b[38;5;241m=\u001b[39madd_records)\n\u001b[0;32m--> 213\u001b[0m     add_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_record_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     add_embeddings \u001b[38;5;241m=\u001b[39m add_records[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py:526\u001b[0m, in \u001b[0;36mCollectionCommon._embed_record_set\u001b[0;34m(self, record_set, embeddable_fields)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\n\u001b[1;32m    523\u001b[0m                 \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader(uris\u001b[38;5;241m=\u001b[39mcast(URIs, record_set[field]))  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecord_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecord does not contain any non-None fields that can be embedded.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddable Fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddable_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecord Fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py:539\u001b[0m, in \u001b[0;36mCollectionCommon._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/guides/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m     )\n\u001b[0;32m--> 539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/api/types.py:466\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 466\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(cast(Embeddings, normalize_embeddings(result)))\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/utils/embedding_functions/onnx_mini_lm_l6_v2.py:200\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Documents) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Only download the model when it is actually used\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_model_if_not_exists()\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Embeddings, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/chromadb/utils/embedding_functions/onnx_mini_lm_l6_v2.py:143\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m    134\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encoded])\n\u001b[1;32m    135\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(input_ids, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(attention_mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     ),\n\u001b[1;32m    142\u001b[0m }\n\u001b[0;32m--> 143\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Perform mean pooling with attention weighting\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/3101_proj/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chromadb_1003\")  # Stores DB in ./chroma_db\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding documents\", unit=\"document\", leave=True, ncols=100):\n",
    "    doc_text = row[\"esg_text\"]  \n",
    "    doc_company = row[\"company\"]  \n",
    "    doc_year = row[\"year\"]  \n",
    "    doc_industry = row[\"industry\"]\n",
    "    doc_id = f\"doc_{index}\"  \n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id], \n",
    "        documents=[doc_text],  \n",
    "        metadatas=[{\"company\": doc_company, \"year\": doc_year}] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5dadc0f-2077-4cf3-81ed-e63b76e2b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(collection.count())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pgVector code\n",
    "pip install pgvector psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b911e81-79af-4529-a8f3-ecfbc0e1ec24",
   "metadata": {},
   "source": [
    "## Testing the Retrieval\n",
    "Method: Semantic similarity to compare embeddings of the query to the sentences, and retrieve the top sentences with the highest similarity scores.\n",
    "Limitations: Different words may have different meanings under different contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41199cad-f303-40cf-bbdf-8a90e8ddd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chromadb_1003\")\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d884077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['doc_0',\n",
       "  'doc_1',\n",
       "  'doc_2',\n",
       "  'doc_3',\n",
       "  'doc_4',\n",
       "  'doc_5',\n",
       "  'doc_6',\n",
       "  'doc_7',\n",
       "  'doc_8',\n",
       "  'doc_9',\n",
       "  'doc_10',\n",
       "  'doc_11',\n",
       "  'doc_12',\n",
       "  'doc_13',\n",
       "  'doc_14',\n",
       "  'doc_15',\n",
       "  'doc_16',\n",
       "  'doc_17',\n",
       "  'doc_18',\n",
       "  'doc_19',\n",
       "  'doc_20',\n",
       "  'doc_21',\n",
       "  'doc_22',\n",
       "  'doc_23',\n",
       "  'doc_24',\n",
       "  'doc_25',\n",
       "  'doc_26'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Contents 000 0 000 0 Origin’s 2024 Sustainability Report outlines how This report follows the SASB Standards and Our reporting suite we are working towards our ambition to lead the references the Global Reporting Initiative (GRI) oe Introduction © energy transition through cleaner energy and 2021 Standards.',\n",
       "  'See our GRI content index and Our Sustainability Report forms part of our customer solutions, focusing on our material SASB index for more information.',\n",
       "  'corporate reporting suite, which includes: sustainability-related risks and opportunities.',\n",
       "  '© 2024 Annual Report A message from the CEO 3 UN Sustainable Development Goals Details our financial and operating performance.',\n",
       "  'wirttteeessescess eee cere cere cree cece eee ee esse esse se eeee esse sseeesseessseses Progress on our Climate Transition Action Plan at .',\n",
       "  '.',\n",
       "  'WaMSIOr Ng) UO Ek Zee = Our policies and actions contribute to several of ° 2024 Corporate Governance Statement i cccccecsstestessecsessessteesessecsessesesseeseeseeseesseeseess We published our Climate Transition Action Plan the United Nations (UN) Sustainable Development Summarises our corporate governance A just energy transition 6 (CTAP) in 2022, which includes our targets across Goals (SDGs).',\n",
       "  'We have mapped these throughout Practices, vcatecsssecsncessteesssccsssecssecssssesscestecstecesteessseesieessecs Scope 1, 2 and 3 emissions to accelerate emissions _ the report and outlined our supporting actions ° Sustainability performance data FY24 performance highlights 7 reduction across our business.',\n",
       "  'See pages 9-18 for on our website.',\n",
       "  'Includes our sustainability performance data for an update on our progress against key elements of the past five years.',\n",
       "  'our CTAP.',\n",
       "  'Assurance oo Planet >] e Sustainability management approaches Reporting frameworks EY has provided limited assurance on certain Describe our approach and policies on how we greenhouse gas emissions and other metrics within manage key aspects of our business.',\n",
       "  'Progress on our CTAP 9 We welcome the introduction of a standardised, this report.',\n",
       "  'Its limited assurance statement includes, 2024 Mod S| Stat SEE, 020000000000000000007000000500000e005070000050909D0qEDHHo7000D000000n0GRoR509C mandatory climate-reporting framework through a list of the data assured.',\n",
       "  '= mrodermn viavery viatement ; Environment 49 the Australian Sustainability Reporting Standards Outlines our processes for identifying, assessing (ASRS) and are preparing our business and Report feedback and addressing the risk of modern slavery in our processes to report against them from FY26.',\n",
       "  'operations and supply chain.',\n",
       "  'Cust ; ; ; We welcome your thoughts and feedback.',\n",
       "  'Email us « 2023 Tax Contribution Report ustomers [> ] In the meantime, we continue to report against the at originsustainability@origin.com.au Details the taxes we paid in FY22 and FY23.',\n",
       "  'Task Force on Climate-related Financial Disclosures Supporting our customers 29 (TCFD) recommendations, and to develop our Find more information on Origin and our reporting ° Industry Association Review 2024 Pporting climate-related financial disclosures, which include at originenergy.com.au Reviews our industry association memberships Eneray reliabilit ee 5A climate sensitivity analysis using a 1.5°C scenario and their respective positions on climate change Mevighy letveloility in our FY24 financial statements.',\n",
       "  'Our TCED index and climate-related policies.',\n",
       "  'outlines where to find our TCFD disclosures within Cc one our reporting suite.',\n",
       "  'ommunities [> | 1.',\n",
       "  'The use by Origin Energy of any MSCI ESG Research LLC or its affiliates (MSCI) data, and the use of MSCI logos, trademarks, service marks or index .',\n",
       "  '9 eng names herein, do not constitute a sponsorship, endorsement, recommendation or promotion of Origin Energy by MSCI.',\n",
       "  'MSCI services and data are Engaging and supporting communities 26 the property of MSCI or its information providers, and are provided as is and without warranty.',\n",
       "  'MSCI names and logos are trademarks or service marks 2.',\n",
       "  'As of February 2023, Origin received an ESG Risk Rating of 34.1 from Morningstar Sustainalytics and was assessed to be at High risk of experiencing Responsible supply chain 31 material financial impacts from ESG factors.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0},\n",
       "  {'company': 'Origin', 'year': 2024.0}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.get()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8642cd89-d3ef-4160-981d-9ca5a5c7a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Retrieve percentage of reduction in Greenhouse gas emissions during the reporting year in the company. This can be in a) Total reduction, b) Scope 1 reduction and c) Scope 2 reduction\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "   where={\n",
    "        \"$and\": [\n",
    "            {\"company\": \"Apple\"},\n",
    "            {\"year\": 2022.0}\n",
    "        ]\n",
    "    },\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb4a89d-dbfe-4790-b740-f6ceba5234b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [[]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[]],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[]],\n",
       " 'distances': [[]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d927e8-1983-44ab-89d2-b039e98ac88f",
   "metadata": {},
   "source": [
    "## Testing the Generator\n",
    "Use DeepSeek API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb83d1f-32ca-484d-a33e-693229822f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fede14eb5954290a3257f7302b5c033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b5f81df9e046d99efe533897c39e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0316d7234d1425a8acd6a27dab7d6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da59c11d09754fb3b232662a503d8c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00179690778143ac9f56719d7afacce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862d7b1d1af647f6aec1382b941ee2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=API_KEY,\n",
    "    http_client=httpx.Client()\n",
    ")\n",
    "reranker_model_name = \"BAAI/bge-reranker-base\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "reranker_model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a395b-1e9d-40d1-afea-357647f78eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, reranked_docs):\n",
    "    \"\"\"Retrieve context from ChromaDB and generate an answer using DeepSeek.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in ESG analysis. Please reason through step by step and then provide the final answer to the query. \n",
    "    Please verify your answer against the context provided, and rewrite the answer if inconsistent. Below is a question and relevant retrieved documents.\n",
    "    \n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Please provide a factually accurate response. If a fact is used from a document, include '(ChunkID)' next to it.\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1-zero:free\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in ESG analysis. Answer factually and ensure consistency with the provided context, especially focusing on environemntal, sustainability and governance principles.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if completion and completion.choices and completion.choices[0].message:\n",
    "                return completion.choices[0].message.content  # Return model's response\n",
    "\n",
    "            print(\"Warning: Empty response. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return \"Error: Unable to generate a response.\"\n",
    "\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved documents based on relevance scores using the BAAI/bge-reranker-base model.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        retrieved_docs (list): A list of retrieved document texts.\n",
    "\n",
    "    Returns:\n",
    "        list: The reranked documents sorted by relevance.\n",
    "    \"\"\"\n",
    "    if not retrieved_docs:\n",
    "        return []\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = reranker_tokenizer(\n",
    "        [query] + retrieved_docs,  \n",
    "        padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute relevance scores\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "    # Sort retrieved docs by relevance score (descending order)\n",
    "    reranked_docs = [doc for _, doc in sorted(zip(scores[1:], retrieved_docs), reverse=True)]\n",
    "\n",
    "    return reranked_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b0fc0-c719-4026-9524-1b6dbd144372",
   "metadata": {},
   "source": [
    "# Start of full RAG code with json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ee38b-56d3-44f6-9440-72040051b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"esg_metrics.json\", \"r\") as file:\n",
    "    esg_metrics = json.load(file)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb_1003\")\n",
    "collection = client.get_or_create_collection(name=\"dsa3101\")\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "df_columns = [\"Company\"] + list(esg_metrics.keys())  # One column per ESG metric\n",
    "df_metrics = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Function to retrieve relevant ESG text using ChromaDB\n",
    "def retrieve_esg_text(company, query):\n",
    "    collection = chroma_client.get_collection(name=\"esgtext\")\n",
    "    results = collection.query(query_texts=[query], n_results=5)\n",
    "    return results\n",
    "\n",
    "# Function to rerank retrieved documents\n",
    "def get_reranked_docs(query, results):\n",
    "    retrieved_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "    return reranked_docs\n",
    "    \n",
    "# Function to extract metric values using DeepSeek  \n",
    "def extract_values(query, retrieved_text):\n",
    "    reranked_docs = get_reranked_docs(query, results)\n",
    "    response = generate_response(query, reranked_docs)\n",
    "    return response[\"text\"]  ######## Adjust based on DeepSeek output format\n",
    "\n",
    "# Function to compute the score based on thresholds\n",
    "def compute_linear_score(extracted_values, thresholds):\n",
    "    ####\n",
    "    pass\n",
    "\n",
    "# List of companies\n",
    "companies = [\"Pfizer\", \"Apple\", \"Datadog\"]  # Replace with actual company list\n",
    "\n",
    "# Process each company\n",
    "for company in companies:\n",
    "    row_data = {\"Company\": company}\n",
    "\n",
    "    for metric, details in esg_metrics.items():\n",
    "        query = details[\"query\"]\n",
    "        scoring_thresholds = details[\"scoring_thresholds\"]\n",
    "\n",
    "        # Retrieve ESG text using ChromaDB\n",
    "        retrieved_text = retrieve_esg_text(company, query)\n",
    "\n",
    "        # Extract values using DeepSeek\n",
    "        extracted_values = extract_values(query, retrieved_text)\n",
    "\n",
    "        # Compute score\n",
    "        score = compute_score(extracted_values, scoring_thresholds)\n",
    "\n",
    "        # Store results\n",
    "        row_data[metric] = {\"extracted_values\": extracted_values, \"score\": score}\n",
    "\n",
    "    # Append to DataFrame\n",
    "    df_metrics = df_metricsdf.append(row_data, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_metrics.to_csv(\"company_esg_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0f764-ef3f-4011-9561-ec60198ce604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655d54a7-83f2-4638-aa9f-2fb069a0f738",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "Checking for hallucination, irrelevance, bias \n",
    "In this assignment, I felt that biasness wasn't really a metric required, I think it would be good to add biasness if i extracted data from third party sources grading the company esg scores. I can then compare the third-party metrics and scoring to each company's esg reports, and check if there is biasness in terms of their ratings, towards a particular, company or industry, etc. Therefore, I just added the metric for future reference, but it is not required in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047809e-b41f-4a4f-a583-94f731705305",
   "metadata": {},
   "source": [
    "### Hallucination detection (Faithfullness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e5914-e633-44c0-996d-1a3471006ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def fuzzy_match(sentence, doc, threshold=80):\n",
    "    \"\"\"Check if sentence has a fuzzy match in the document.\"\"\"\n",
    "    return fuzz.partial_ratio(normalize_text(sentence), normalize_text(doc)) >= threshold\n",
    "\n",
    "def verify_facts(response, reranked_docs, fuzzy_threshold=80):\n",
    "    \"\"\"Detect hallucinations by checking if sentences exist in retrieved docs using fuzzy matching.\"\"\"\n",
    "    missing_facts = []\n",
    "    \n",
    "    # Split response into sentences and check if they appear in any of the documents\n",
    "    for sent in response.split(\". \"):\n",
    "        found = any(fuzzy_match(sent, doc, fuzzy_threshold) for doc in reranked_docs)\n",
    "        if not found:\n",
    "            missing_facts.append(sent)\n",
    "\n",
    "    if missing_facts:\n",
    "        print(\"Warning: Some statements are not found in the retrieved context:\")\n",
    "        for fact in missing_facts:\n",
    "            print(f\"- {fact}\")\n",
    "    \n",
    "    return 1 - len(missing_facts) / len(response.split(\". \"))  # Faithfulness Score\n",
    "\n",
    "faithfulness_score = verify_facts(response, reranked_documents)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e383-f6c2-4755-86c6-db9fcfc00bc5",
   "metadata": {},
   "source": [
    "## Irrelevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda28345-5526-41e5-8d04-1e3cb97f9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def check_relevance(query, response, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Check the relevance of the response to the query using semantic similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    response_embedding = model.encode([response])\n",
    "\n",
    "    similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        return True, similarity  \n",
    "    else:\n",
    "        return False, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c8dd-2b62-4e30-bbe9-51973dacffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_relevant = check_relevance(query, response)\n",
    "print(f\"Is the response relevant? {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c597bf1-8a94-4e40-99da-74907eaa51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-reranker-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def check_bias(text):\n",
    "    \"\"\"\n",
    "    Check for potential bias in the text using a pretrained model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Assuming binary classification (0 = no bias, 1 = biased)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class == 1  # 1 indicates bias (this depends on the model's labeling)\n",
    "\n",
    "# Example Usage\n",
    "response = \"Pfizer has been focusing on improving diversity in their clinical trials and sharing their insights with others as part of their diversity and inclusion initiatives in 2022.\"\n",
    "\n",
    "is_biased = check_bias(response)\n",
    "print(f\"Is the response biased? {is_biased}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb7396-1af0-423d-8676-a48c8516eb68",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa267f2-b321-4758-afac-a20d44832149",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "Typical metrics: RecalL@k, Precision @k, Mean Reciprocal Rank, Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc47e47-9939-43d9-8c8d-e0130e7be26f",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5510ac0-4b18-4606-a1b4-b155d3634525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(query, reranked_docs):\n",
    "    \"\"\"Compute semantic similarity between query and retrieved docs.\"\"\"\n",
    "    corpus = [query] + reranked_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    return similarity_scores.mean()  # Average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71078-d266-46bf-9106-943f2dd30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(query, reranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f70ad-88b8-4692-a72c-775ba45db831",
   "metadata": {},
   "source": [
    "## Generator Evaluation \n",
    "Typical metrics: ROUGE, BLEU, BERTScore, domain-specific or task-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410181f7-bc69-415c-991c-5461bdb5b27b",
   "metadata": {},
   "source": [
    "### BLEU Score (Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9b46e-b114-427a-a366-d5c22324a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(reference, generated_response):\n",
    "    \"\"\"Compare generated response against reference text using BLEU score.\"\"\"\n",
    "    reference_tokens = reference.lower().split()\n",
    "    generated_tokens = generated_response.lower().split()\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5aae-8086-4306-b318-d697e93cccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_bleu_score(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb8f94-3737-4fcf-b003-74a17bfa839f",
   "metadata": {},
   "source": [
    "### Retrieval score (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6208a-09cd-4eb8-9523-b78e3152a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_retrieval_relevance(reranked_docs, response):\n",
    "    \"\"\"Calculate how relevant the response is to the retrieved documents.\"\"\"\n",
    "    corpus = reranked_docs + [response]  # Combine all docs and response\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    return similarity_matrix.mean()  # Average similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c430da-47b0-464a-a510-d5a3ac0c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retrieval_relevance(reranked_docs, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4a61-3080-4ed3-929f-6560fe92648a",
   "metadata": {},
   "source": [
    "### Judge LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3e25-334e-4c48-82f4-d9a27b677207",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=google_api_key)\n",
    "reranked_docs_str = \"\\n\".join(reranked_documents)\n",
    "\n",
    "gemini_eval = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"\"\"\n",
    "                Evaluate how well the response answers the query, giving an explanation of how it answers the question, and whether the response is factually correct based on the context provided.\n",
    "                I have added the query, response and retrieved context below.\n",
    "                \n",
    "                Query: \n",
    "                {query}\n",
    "                \n",
    "                Response:\n",
    "                {response}\n",
    "                \n",
    "                Retrieved Context:\n",
    "                {reranked_docs_str}\n",
    "                \n",
    "                Give a score from 0 to 10, and a detailed explanation on the score, where:\n",
    "                - 10 = Perfectly accurate\n",
    "                - 0 = Completely incorrect\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "print(gemini_eval.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3101_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
